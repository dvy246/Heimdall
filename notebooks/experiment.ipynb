{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fdb7352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/divyyadav/miniforge3/envs/newenv/lib/python3.10/site-packages/pandas_ta/__init__.py:7: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution, DistributionNotFound\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langsmith import traceable\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from typing import Annotated,TypedDict,Literal,Dict,List,Any,Optional\n",
    "from pydantic import BaseModel,Field\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage,BaseMessage\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import create_react_agent,tools_condition,ToolNode\n",
    "from finnhub import Client\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "import pandas as pd\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from pathlib import Path\n",
    "import os\n",
    "from langchain_core.tools import tool\n",
    "import sys\n",
    "import time \n",
    "import shutil\n",
    "from langgraph_supervisor import create_supervisor,create_handoff_tool\n",
    "from langgraph_swarm import create_swarm,create_handoff_tool\n",
    "from datetime import datetime,timedelta\n",
    "from sec_api import QueryApi\n",
    "import requests\n",
    "from sec_edgar_api import EdgarClient\n",
    "import json\n",
    "import pandas_ta as ta\n",
    "import os\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import sqlite3\n",
    "import logging\n",
    "import regex as re\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11f83d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ChatGroq(api_key=os.getenv(\"GROQ\"),model=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a82f3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sentiment(BaseModel):\n",
    "    sentiment:Literal['positive','negative','neutral']=Field(...,description=\"you need to classify the sentiment of the message based on the analysis\")\n",
    "    reason:str =Field(...,description=\"you need to give the reason for the sentiment classification\")\n",
    "    overall_factors:Dict[str,str]=Field(...,description=\"Factors influencing the sentiment in which the key is sentiment and reason is the value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d894443",
   "metadata": {},
   "outputs": [],
   "source": [
    "class reason_setiment(BaseModel):\n",
    "    overall_sentiment:Literal['positive','negative','neutral']=Field(...,description=\"you need to classify the sentiment of the message based on the analysis\")\n",
    "    analysis: Annotated[List, Field(min_items=1, description=\"List of bullet points explaining the sentiment classification.\")] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b77cda7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_wstr_output=model.with_structured_output(reason_setiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eed832fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_structure=model.with_structured_output(Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "896aa19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockAgent(TypedDict):\n",
    "    ticker:str\n",
    "    message:Annotated[list,add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b896b063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_web(query:str):\n",
    "   ''' Searches the web for the given query and returns the top 10 results '''\n",
    "   tavily=TavilySearchResults(max_results=20,api_key=os.getenv('TAVILY_API_KEY'))\n",
    "   return tavily.invoke(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "982746c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sec_api import QueryApi # Changed from EdgarClient\n",
    "from bs4 import BeautifulSoup\n",
    "import requests # Added import\n",
    "\n",
    "# IMPORTANT: The SEC API requires you to set a User-Agent.\n",
    "# Replace 'Your Name' and 'your.email@example.com' with your actual info.\n",
    "os.environ['EDGAR_USER_AGENT'] = 'Divy yadavdipu296@gmail.com'\n",
    "@tool(description=\"Fetches the full text of the most recent 10-K filing for a given company ticker using sec_api.QueryApi.\")\n",
    "def get_latest_10k_filing(ticker: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the full text of the most recent 10-K filing for a given company ticker using sec_api.QueryApi.\n",
    "\n",
    "    Args:\n",
    "        ticker: The company's stock ticker (e.g., \"TSLA\").\n",
    "\n",
    "    Returns:\n",
    "        The plain text content of the 10-K filing's primary document.\n",
    "        Returns an error message if the filing cannot be fetched.\n",
    "    \"\"\"\n",
    "    print(f\"---  SEC TOOL: Fetching latest 10-K for {ticker} ---\")\n",
    "    try:\n",
    "        # Initialize QueryApi with your SEC API key\n",
    "        query_api = QueryApi(api_key=os.getenv('SEC_API_KEY')) # Assumes SEC_API_KEY in .env\n",
    "\n",
    "        # Query for the latest 10-K filing for the ticker\n",
    "        query = {\n",
    "        \"query\": {\"query_string\": {\"query\": f'ticker:{ticker} AND formType:\"10-K\"'}},\n",
    "        \"from\": 0,\n",
    "        \"size\": 1,\n",
    "        \"sort\": [{\"filedAt\": {\"order\": \"desc\"}}],\n",
    "    }\n",
    "        response = query_api.get_filings(query=query)\n",
    "        \n",
    "        if not response or not response.get(\"filings\"):\n",
    "            return f\"Error: No 10-K filings found for ticker {ticker}.\"\n",
    "\n",
    "        # Get the URL of the primary document from the latest filing\n",
    "        filing_url = response[\"filings\"][0].get(\"linkToHtml\")\n",
    "        \n",
    "        if not filing_url:\n",
    "            return f\"Error: Could not find a filing URL for {ticker}.\"\n",
    "\n",
    "        # Fetch the content of the filing URL using requests\n",
    "        filing_html = requests.get(filing_url, headers={'User-Agent': os.getenv('EDGAR_USER_AGENT')}).text\n",
    "\n",
    "        # Use BeautifulSoup to parse the HTML and extract all text\n",
    "        soup = BeautifulSoup(filing_html, 'html.parser')\n",
    "        plain_text = soup.get_text(separator='\\n', strip=True)\n",
    "\n",
    "        print(f\"--- SEC TOOL: Successfully fetched and parsed 10-K for {ticker}. ---\")\n",
    "        return plain_text\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred while fetching the SEC data: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a99fd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(description=\"Searches the web for the the ticker and gives the result sentiment with analysis\")\n",
    "def analyze_news_sentiment(ticker:str,days_back:int=30)-> Dict[str, Any]:\n",
    "                    \"\"\"\n",
    "                    Analyzes recent news sentiment for a given company ticker over a specified period.\n",
    "                    \n",
    "                    Args:\n",
    "                        ticker: The company's stock ticker (e.g., \"TSLA\").\n",
    "                        days_back: Number of days to look back for news articles (default: 30).\n",
    "                        \n",
    "                    Returns:\n",
    "                        A dictionary containing news sentiment analysis.\n",
    "                    \"\"\"\n",
    "                    try:\n",
    "                        end_date=datetime.now()\n",
    "                        start_date=end_date-timedelta(days=days_back)\n",
    "                        search_query = f\"{ticker} company news from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')} of its performances and changes\"\n",
    "                        news_result= search_web(search_query)\n",
    "                        if not news_result:\n",
    "                            return {'error':f\" no articles found for the ticker {ticker} for the specific period\"}\n",
    "                        news_text=''\n",
    "                        for i,result in enumerate(news_result[:10],1):\n",
    "                            news_text += f\"Article {i}: {result.get('title', 'No title')}\\n\"\n",
    "                            news_text += f\"Content: {result.get('content', 'No content')[:200]}...\\n\\n\"\n",
    "                    except Exception as e:\n",
    "                        raise {'error':f\"An error occurred while fetching news articles: {e}\"}\n",
    "                    sentiment_result=model_with_structure.invoke(news_text)\n",
    "                    prompt=f\"based on the {sentiment_result} provide a summary of the news sentiment with detailed reasons from the news text\"\n",
    "                    final_sentiment_with_reason=model2_wstr_output.invoke(prompt)\n",
    "                    return final_sentiment_with_reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c054cf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(description=\"gets the info of the particular companys overall functions\")\n",
    "async def company_overview(ticker:str):\n",
    "    '''gets the info of the particular companys overall functions with all the metrics of the company'''\n",
    "    api=os.getenv('Alpha_Vantage_Stock_API')\n",
    "    if not api:\n",
    "        return {'API KEY IS MISSING'}\n",
    "    try:\n",
    "        url=f'https://www.alphavantage.co/query?function=OVERVIEW&symbol={ticker}&apikey={api}'\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(url) as response:\n",
    "                response.raise_for_status()\n",
    "                data=await response.json()\n",
    "        if \"Note\" in data or not data:\n",
    "                return {\"error\": f\"Could not fetch data for {ticker}. The API limit may be reached or the ticker is invalid.\"}\n",
    "\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        return {'error':f\"An error occurred while fetching the data: {e}\"}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {'error':f\"An error occurred while fetching the data: {e}\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e505996",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tool(description='gets the insider infor of the company')\n",
    "def get_insider_info(ticker:str):\n",
    "        ''' returns the latest and historical insider transactions made by key stakeholders (e.g., founders, executives, board members, etc.) of a specific company.'''\n",
    "\n",
    "        api=os.getenv('Alpha_Vantage_Stock_API')\n",
    "        if not api:\n",
    "           return {'API KEY IS MISSING'}\n",
    "        url=f'https://www.alphavantage.co/query?function=INSIDER_TRANSACTIONS&symbol={ticker}&apikey={api}'\n",
    "        try:\n",
    "            response=requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            data=response.json()\n",
    "            if \"Note\" in data or not data:\n",
    "                return {\"error\": f\"Could not fetch data for {ticker}. The API limit may be reached or the ticker is invalid.\"}\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            return {'error':f\"An error occurred while fetching the data: {e}\"}\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return {'error':f\"An error occurred while fetching the data: {e}\"}\n",
    "\n",
    "        \n",
    "@tool(description='does advanced analytics details of the company')\n",
    "def advaced_analyst(ticker:list[str],\n",
    "    interval: str,\n",
    "    window_size: int,\n",
    "    calculations: List[str],\n",
    "    range_str: str,\n",
    "    ohlc: Optional[str] = \"close\"\n",
    "):\n",
    "        ''' This endpoint returns a rich set of advanced analytics metrics (e.g., total return, variance, auto-correlation, etc.) for a given time series over sliding time windows. For example, we can calculate a moving variance over 5 years with a window of 100 points to see how the variance changes over time.'''\n",
    "        api=os.getenv('Alpha_Vantage_Stock_API')\n",
    "        if not api:\n",
    "               return {\"error\": \"Alpha Vantage API key not found.\"}\n",
    "            \n",
    "        base_url=\"https://www.alphavantage.co/query\"\n",
    "        params={\n",
    "            \"function\": \"ANALYTICS_SLIDING_WINDOW\",\n",
    "            'SYMBOLS':ticker,\n",
    "            \"RANGE\": '6month',\n",
    "            \"INTERVAL\": interval,\n",
    "            \"WINDOW_SIZE\": window_size,\n",
    "            \"CALCULATIONS\": \",\".join(calculations),\n",
    "            \"OHLC\": ohlc,\n",
    "            \"apikey\": api\n",
    "        }\n",
    "            \n",
    "        try:\n",
    "            # Use the list of tuples for the request parameters\n",
    "            response = requests.get(base_url, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            if \"Note\" in data or not data:\n",
    "                return {\"error\": \"Could not fetch data. API limit may be reached or parameters are invalid.\"}\n",
    "            \n",
    "            return data.get(\"payload\", {\"error\": \"No payload found.\"})\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return {\"error\": f\"An API request error occurred: {e}\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"An unexpected error occurred: {e}\"}\n",
    "@tool(description=\"Fetches the current market status for a given exchange using the Finnhub API.\")\n",
    "def get_market_status(exchange: str) -> str:\n",
    "    ''' Fetches the current market status for a given exchange using the Finnhub API.'''\n",
    "    try:\n",
    "        finnhub_client = Client(api_key=os.getenv(\"FINNHUB_API_KEY\"))\n",
    "        get_market_status=finnhub_client.market_status(exchange='US')\n",
    "        return get_market_status\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"An unexpected error occurred: {e}\"}\n",
    "\n",
    "@tool(description=\"Fetches insider sentiment data for a given stock ticker within a specified date range.\")\n",
    "def get_insiders_sentiment(ticker: str) -> Dict[str, Any]:\n",
    "    '''Fetches insider sentiment data for a given stock ticker within a specified date range.'''\n",
    "    try:\n",
    "        finnhub_client = Client(api_key=os.getenv(\"FINNHUB_API_KEY\"))\n",
    "        end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        start_date = (datetime.now() - timedelta(days=90)).strftime('%Y-%m-%d')\n",
    "\n",
    "        sentiment_data = finnhub_client.stock_insider_sentiment(ticker, start_date, end_date)\n",
    "        if not sentiment_data or 'data' not in sentiment_data:\n",
    "            return {\"message\": f\"No insider sentiment data found for {ticker} in the last 90 days.\"}\n",
    "        total_mspr=sum(item['mspr'] for item in sentiment_data['data'])\n",
    "        sentiment_score = \"Positive\" if total_mspr > 0 else \"Negative\" if total_mspr < 0 else \"Neutral\"\n",
    "\n",
    "        return {\n",
    "            \"ticker\": ticker,\n",
    "            \"monthly_sentiment_score_mspr\": total_mspr,\n",
    "            \"overall_sentiment\": sentiment_score\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"An unexpected error occurred: {e}\"} \n",
    "@tool(description=\"Use this tool to get the current date.\")\n",
    "def get_current_date() -> str:\n",
    "    \"\"\"Returns the current date in YYYY-MM-DD format.\"\"\"\n",
    "    return datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "@tool(description=\"\"\"\n",
    "    Use this tool to get key technical indicators (RSI, 50-day SMA, 200-day SMA) \n",
    "    for a given stock ticker. The input MUST be a single company stock ticker.\n",
    "    For example: 'AAPL' or 'TSLA'.\n",
    "\"\"\")\n",
    "def get_technichal_analysis(ticker: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fetches historical stock data and calculates key technical indicators.\n",
    "        Returns a dictionary with the current price and indicators, or an error message.\n",
    "        \"\"\"\n",
    "        print(f\"--- TECHNICAL TOOL: Performing full analysis for {ticker} ---\")\n",
    "        try:\n",
    "            stock=yf.Ticker(ticker)\n",
    "            hist = stock.history(period=\"1y\")\n",
    "            \n",
    "            if hist.empty:\n",
    "                return f\"Error: No historical data found for ticker {ticker}.\"\n",
    "            \n",
    "            hist.columns=[col.capitalize() for col in hist.columns]\n",
    "            \n",
    "            #calculating metrics\n",
    "            hist.ta.rsi(append=True)\n",
    "            hist.ta.sma(length=50, append=True)\n",
    "            hist.ta.sma(length=200, append=True)\n",
    "\n",
    "            if 'RSI_14' not in hist.columns or 'SMA_50' not in hist.columns or 'SMA_200' not in hist.columns:\n",
    "                      return {\"error\": f\"Could not calculate all technical indicators for {ticker}. Not enough data.\"}\n",
    "                      \n",
    "            latest_indicators = {\n",
    "            \"current_price\": hist['Close'].iloc[-1],\n",
    "            \"rsi\": hist['RSI_14'].iloc[-1],\n",
    "            \"sma_50\": hist['SMA_50'].iloc[-1],\n",
    "            \"sma_200\": hist['SMA_200'].iloc[-1]\n",
    "        }\n",
    "\n",
    "            for key,value in latest_indicators.items():\n",
    "                if pd.isna(value):\n",
    "                    return {\"error\": f\"Could not calculate {key} for {ticker}. Not enough data.\"}\n",
    "            return latest_indicators\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"An error occurred while fetching the technical analysis: {e}\"\n",
    "\n",
    "@tool(description='gives back the income statement of the company')\n",
    "async def get_income_statements(ticker: str,period:Literal\n",
    "['annual','quarter']) -> Dict:\n",
    "    \"\"\"\n",
    "    Fetches the income statement of the specified company ticker.\n",
    "\n",
    "    Args:\n",
    "        ticker (str): The stock ticker symbol of the company.\n",
    "        period (str): The period for the income statement, either 'annual' or 'quarter'.\n",
    "\n",
    "    Returns:\n",
    "        Dict: A dictionary containing the income statement data of the company,\n",
    "              or an error message if the API call fails or the API key is missing.\n",
    "\n",
    "    Description:\n",
    "        This tool queries the Financial Modeling Prep API to retrieve the income statement\n",
    "        for the given ticker symbol and period. It requires the 'FPREP' environment variable\n",
    "        to be set with a valid API key.\n",
    "    \"\"\"\n",
    "    api = os.getenv(\"FPREP\")\n",
    "    if not api:\n",
    "        return {\"error\": \"API key not found. Please set the 'FPREP' environment variable.\"}\n",
    "    \n",
    "    url = f\"https://financialmodelingprep.com/stable/income-statement?symbol={ticker}&apikey={api}&period=annual\"\n",
    "    \n",
    "    try:\n",
    "        async with aiohttp.ClientSession() as session:   # âœ… ensures session is closed\n",
    "            async with session.get(url) as response:\n",
    "                if response.status != 200:\n",
    "                    return {\"error\": f\"Failed with status {response.status}\"}\n",
    "                data = await response.json()\n",
    "                result= json.dump(data[0],fp=sys.stdout,indent=4)\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"An unexpected error occurred: {str(e)}\"}\n",
    "\n",
    "@tool(description='gives back the cashflow of the company')\n",
    "async def get_cashflow(ticker:str,period:Literal\n",
    "['annual','quarter'])->Dict:\n",
    "    \"\"\"\n",
    "    Fetches the cash flow statement of the specified company ticker.\n",
    "\n",
    "    Args:\n",
    "        ticker (str): The stock ticker symbol of the company.\n",
    "        period (str): The period for the cash flow statement, either 'annual' or 'quarter'.\n",
    "\n",
    "    Returns:\n",
    "        Dict: A dictionary containing the cash flow statement data of the company,\n",
    "              or an error message if the API call fails or the API key is missing.\n",
    "\n",
    "    Description:\n",
    "        This tool queries the Financial Modeling Prep API to retrieve the cash flow statement\n",
    "        for the given ticker symbol and period. It requires the 'FPREP' environment variable\n",
    "        to be set with a valid API key.\n",
    "    \"\"\"\n",
    "    api = os.getenv(\"FPREP\")\n",
    "    if not api:\n",
    "        return {\"error\": \"API key not found. Please set the 'FPREP' environment variable.\"}\n",
    "    url=f'https://financialmodelingprep.com/api/v3/cash-flow-statement/{ticker}?period={period}&apikey={api}'\n",
    "    try:\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(url) as response:\n",
    "                if response.status != 200:\n",
    "                    return {\"error\": f\"Failed with status {response.status}\"}\n",
    "                data = await response.json()\n",
    "                return data\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"An unexpected error occurred: {str(e)}\"}\n",
    "@tool(description='gives back the balance sheet of the company')\n",
    "async def get_balance_sheet(ticker:str,period:str='annual'):\n",
    "        \"\"\"\n",
    "        Fetches the balance sheet of the specified company ticker.\n",
    "\n",
    "        Args:\n",
    "            ticker (str): The stock ticker symbol of the company.\n",
    "            period (str, optional): The period for the balance sheet, either 'annual' or 'quarter'. Defaults to 'annual'.\n",
    "\n",
    "        Returns:\n",
    "            Dict: A dictionary containing the balance sheet data of the company,\n",
    "                  or an error message if the API call fails or the API key is missing.\n",
    "\n",
    "        Description:\n",
    "            This tool queries the Alpha Vantage API to retrieve the balance sheet data for the given ticker symbol.\n",
    "            It requires the 'Alpha_Vantage_Stock_API' environment variable to be set with a valid API key.\n",
    "        \"\"\"\n",
    "        api=os.getenv('Alpha_Vantage_Stock_API')\n",
    "        if not api:\n",
    "            return {\"error\": \"API key not found. Please set the 'Alpha_Vantage_Stock_API' environment variable.\"}\n",
    "        url=f'https://www.alphavantage.co/query?function=BALANCE_SHEET&symbol={ticker}&apikey={api}&period={period}'\n",
    "        try:\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async  with session.get(url) as response:\n",
    "                    if response.status != 200:\n",
    "                        return {\"error\": f\"Failed with status {response.status}\"}\n",
    "                    data = await response.json()\n",
    "                    for i in data['annualReports']:\n",
    "                        print(i)\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"An unexpected error occurred: {str(e)}\"}\n",
    "\n",
    "@tool(description='gives back the key metrics of the company') \n",
    "async def get_eearning(ticker:str)->Dict:\n",
    "                \"\"\"\n",
    "                Fetches the key financial metrics (earnings) of the specified company ticker.\n",
    "\n",
    "                Args:\n",
    "                    ticker (str): The stock ticker symbol of the company.\n",
    "\n",
    "                Returns:\n",
    "                    Dict: A dictionary containing the key metrics (annual earnings) of the company,\n",
    "                          or an error message if the API call fails or the API key is missing.\n",
    "\n",
    "                Description:\n",
    "                    This tool queries the Alpha Vantage API to retrieve the annual earnings data for the given ticker symbol.\n",
    "                    It requires the 'Alpha_Vantage_Stock_API' environment variable to be set with a valid API key.\n",
    "                \"\"\"\n",
    "                api=os.getenv('Alpha_Vantage_Stock_API')\n",
    "                if not api:\n",
    "                    return {\"error\": \"API key not found. Please set the 'Alpha_Vantage_Stock_API' environment variable.\"}\n",
    "                url=f'https://www.alphavantage.co/query?function=EARNINGS&symbol={ticker}&apikey={api}'\n",
    "                try:\n",
    "                    async with aiohttp.ClientSession() as session:\n",
    "                        async with session.get(url) as response:\n",
    "                            if response.status != 200:\n",
    "                                return {\"error\": f\"Failed with status {response.status}\"}\n",
    "                            data = await response.json()\n",
    "                            for i in data['annualEarnings']:\n",
    "                                            print(i)\n",
    "                except Exception as e:\n",
    "                    return {\"error\": f\"An unexpected error occurred: {str(e)}\"}\n",
    "\n",
    "@tool(description='a tool to get current news')\n",
    "def get_current_markettrends():\n",
    "    \"\"\"Get broad market trends for planning.\"\"\"\n",
    "    query = f\"Current market trends for stocks as of {get_current_date()}\"\n",
    "    return search_web(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c387ada",
   "metadata": {},
   "source": [
    "# ADVANCED RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd3dabdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Validate(BaseModel):\n",
    "    is_correct: bool = Field(..., description='True if the answer is supported by context, False otherwise')\n",
    "\n",
    "model_y_n = model.with_structured_output(Validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "749bc083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartFilterWrapper:\n",
    "    def __init__(self,retriever_base):\n",
    "        self.retriever=retriever_base\n",
    "    def get_context(self,query:str):\n",
    "        try:\n",
    "            relevant_docs=self.retriever.get_relevant_documents(query)\n",
    "            context='\\n\\n'.join([d.page_content for d in relevant_docs] )\n",
    "            return context\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while filtering documents: {e}\")\n",
    "    def filter_documents(self,query:str):\n",
    "        relevant_documents=self.retriever.get_relevant_documents(query)\n",
    "        filtered_docs=[]\n",
    "\n",
    "        for docs in relevant_documents:\n",
    "            content=docs.page_content\n",
    "            score=0\n",
    " \n",
    "            if re.search(r'\\$[\\d,]+|[\\d,]+%|\\d+\\.\\d+', content):\n",
    "                score += 2\n",
    "            \n",
    "            # Boost if longer content (more detailed)\n",
    "            if len(content.split()) > 50:\n",
    "                score += 1\n",
    "                \n",
    "            # Penalize very short content\n",
    "            if len(content.split()) < 20:\n",
    "                score -= 1\n",
    "\n",
    "            filtered_docs.append((docs,score))\n",
    "\n",
    "        sorted_docs = sorted(filtered_docs, key=lambda x: x[1], reverse=True)\n",
    "        final_docs=[doc for doc,score in sorted_docs[:5]]\n",
    "\n",
    "        return final_docs\n",
    "        \n",
    "def clean_financial_text(text: str) -> str:\n",
    "    \"\"\"Clean up messy financial text\"\"\"\n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Fix common OCR errors in financial docs\n",
    "    text = re.sub(r'(\\d),(\\d)', r'\\1,\\2', text)  # Fix number formatting\n",
    "    return text.strip()\n",
    "\n",
    "def better_chunking(text: str):\n",
    "    \"\"\"Chunk text using semantic if short, otherwise recursive splitter\"\"\"\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(\n",
    "        model=\"models/embedding-001\",\n",
    "        google_api_key=os.getenv(\"google\")\n",
    "\n",
    "    )\n",
    "    text=clean_financial_text(text=text)\n",
    "    try:\n",
    "        if len(text.split()) < 800:\n",
    "            semantic = SemanticChunker(embeddings=embeddings)\n",
    "            chunks = semantic.split_text(text)\n",
    "            print(f\"âœ… Used semantic chunking: {len(chunks)} chunks\")\n",
    "        else:\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=800,\n",
    "                chunk_overlap= 100\n",
    "            )\n",
    "            chunks = text_splitter.split_text(text)\n",
    "            print(f\"âš ï¸ Used recursive chunking: {len(chunks)} chunks\")\n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while chunking the text: {e}\")\n",
    "        return [text]\n",
    "\n",
    "def chunk_metada(chunks:list[str],ticker:str):\n",
    "    meta_data_chunks=[]\n",
    "    for i,chunk in enumerate(chunks):\n",
    "        doc=Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                \"chunk_id\": i,\n",
    "                \"ticker\": ticker,\n",
    "                'word_count': len(chunk.split()),\n",
    "                \"contains_numbers\": bool(re.search(r'\\d+', chunk))\n",
    "            }\n",
    "        )\n",
    "        meta_data_chunks.append(doc)\n",
    "    return meta_data_chunks\n",
    "\n",
    "def enhance_query(original_query: str,ticker:str) -> str:  \n",
    "        \"\"\"Enhance the query for better retrieval from a financial document.\"\"\"\n",
    "        detected=detect_query_typ(original_query)\n",
    "        enhancement_prompt = f\"\"\"\n",
    "                You are a financial analysis expert. Enhance this query for better document retrieval.\n",
    "                \n",
    "                Original query: \"{original_query}\"\n",
    "                Company ticker: {ticker}\n",
    "                Query type: {detected}\n",
    "                \n",
    "                Enhancement rules:\n",
    "                - If asking about revenue: include terms like \"net sales\", \"total revenue\", \"operating income\"\n",
    "                - If asking about debt: include \"liabilities\", \"borrowings\", \"credit facilities\"\n",
    "                - If asking about risks: include \"risk factors\", \"uncertainties\", \"challenges\"\n",
    "                - Always include the company ticker\n",
    "                - Make it specific but not too long\n",
    "                - make sure the ouput aint that long which is easy \n",
    "                \n",
    "                Enhanced query:\"\"\"\n",
    "        try:\n",
    "                enhanced_query = model.invoke(enhancement_prompt).content.strip()\n",
    "                return enhanced_query\n",
    "        except:\n",
    "                return original_query\n",
    "\n",
    "\n",
    "def create_context_citations(docs):\n",
    "    \"\"\"Create context with citations for traceability\"\"\"\n",
    "    context = []\n",
    "    for i, doc in enumerate(docs, 1):  # Fixed: (i, doc) not (doc, i) and start from 1\n",
    "        cited_content = f\"[Source {i}] {doc.page_content}\"\n",
    "        context.append(cited_content)\n",
    "    return \"\\n\\n\".join(context)\n",
    "\n",
    "def detect_query_typ(query:str):\n",
    "    try:\n",
    "        query_lower=query.lower().strip()\n",
    "        if any(word in query_lower for word in ['revenue', 'sales', 'income']):\n",
    "                                   return \"financial_performance\"\n",
    "        elif any(word in query_lower for word in ['debt', 'liability', 'borrowing']):\n",
    "                                     return \"financial_position\"\n",
    "        elif any(word in query_lower for word in ['risk', 'challenge', 'uncertainty']):\n",
    "                                        return \"risk_analysis\"\n",
    "        else:\n",
    "            return \"general_inquiry\"\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while detecting query type: {e}\")\n",
    "        return \"general_inquiry\"\n",
    "\n",
    "\n",
    "def create_hybrid_retreiver(vectorstore):\n",
    "    \"\"\"Create hybrid retriever using BM25 + Vector similarity\"\"\"\n",
    "    try:\n",
    "        # Vector retriever\n",
    "        vector_retriever = vectorstore.as_retriever(\n",
    "            search_kwargs={\"k\": 10}, search_type=\"similarity\"\n",
    "        )\n",
    "\n",
    "        # Pull docs from vector retriever\n",
    "        docs = vectorstore.get()[\"documents\"]  \n",
    "        docs = [Document(page_content=d) for d in docs]\n",
    "\n",
    "        if docs:\n",
    "            bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "            bm25_retriever.k = 10\n",
    "\n",
    "            # Ensemble retriever (hybrid)\n",
    "            ensemble = EnsembleRetriever(\n",
    "                retrievers=[vector_retriever, bm25_retriever],\n",
    "                weights=[0.6, 0.4]\n",
    "            )\n",
    "\n",
    "            return SmartFilterWrapper(ensemble)\n",
    "            \n",
    "        return SmartFilterWrapper(vector_retriever)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while creating hybrid retriever: {e}\")\n",
    "        return vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "\n",
    "def answer_validator(enhanced_query:str,answer:str,retriever,context:str):\n",
    "                    ''' the job of this function is to validate the answer of the query from the vector store and then matching the context and if they are unsimilar then provide with the new context for the model to answer from '''\n",
    "                    try:\n",
    "                        validation_prompt = f\"\"\"\n",
    "                        You are a validation expert. Your job is to determine if the provided 'Answer' is fully supported by the 'Context'.\n",
    "                        Read the context carefully and then read the answer. Respond with only 'CORRECT' or 'INCORRECT'.\n",
    "\n",
    "                        Context:\n",
    "                        ---\n",
    "                        {context}\n",
    "                        ---\n",
    "                        Answer:\n",
    "                        ---\n",
    "                        {answer}\n",
    "                        ---\n",
    "                        \"\"\"\n",
    "                        output=model_y_n.invoke(validation_prompt)\n",
    "                        if output==True:\n",
    "                            return answer\n",
    "                        else:\n",
    "                            print('Making sure that the answer matches the context')\n",
    "                            new_docs=retriever.filter_documents(enhanced_query)\n",
    "                            new_context='\\n\\n'.join([doc.page_content for doc in new_docs])\n",
    "                            retry_prompt = f\"\"\"The previous answer was not supported by its context. Please try again.\n",
    "                                Answer the following question based ONLY on the new, updated context provided.\n",
    "\n",
    "                                Context:\n",
    "                                {new_context}\n",
    "\n",
    "                                Question: {enhanced_query}\n",
    "\n",
    "                                Answer:\n",
    "                                \"\"\"\n",
    "                            new_answer = model.invoke(retry_prompt).content\n",
    "                            return new_answer\n",
    "\n",
    "                    except Exception as e:\n",
    "                            print(f\"An error occurred during validation: {e}\")\n",
    "                            return answer # If validation fails, return the original answer\n",
    "\n",
    "@tool(description='ingestes the report data into a vector store')\n",
    "def ingest_10k_filling(report_text: str, ticker: str):\n",
    "    \"\"\"\n",
    "    Ingests the information of a company and stores it into a vector store for later retrieval.\n",
    "\n",
    "    Args:\n",
    "        report_text (str): The full text of the 10-K filing to be ingested.\n",
    "        ticker (str): The stock ticker symbol of the company.\n",
    "\n",
    "    Side Effects:\n",
    "        - Creates a directory for the ticker under \"INDEXED\" if it does not exist.\n",
    "        - Chunks the report text and generates embeddings.\n",
    "        - Stores the embeddings in a persistent Chroma vector store.\n",
    "        - Prints the path where vectors are saved.\n",
    "        - Prints an error message if ingestion fails.\n",
    "    \"\"\"\n",
    "    # Define the path where the indexed data will be stored\n",
    "    path = Path(\"INDEXED\") / ticker\n",
    "    \n",
    "    # Clean up any existing directory to avoid permission issues\n",
    "    if path.exists():\n",
    "        try:\n",
    "            # On macOS, we need to ensure we have proper permissions\n",
    "            shutil.rmtree(path)\n",
    "            print(f\"ðŸ§¹ Removed existing directory: {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not remove existing directory: {e}\")\n",
    "            # Try to fix permissions\n",
    "            try:\n",
    "                os.system(f\"chmod -R 755 {path}\")\n",
    "                shutil.rmtree(path)\n",
    "                print(f\"ðŸ§¹ Removed existing directory after fixing permissions: {path}\")\n",
    "            except:\n",
    "                return f\"Error: Could not remove existing directory: {e}\"\n",
    "    \n",
    "    try:\n",
    "        # Create directory with proper permissions\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Set proper permissions on macOS\n",
    "        os.system(f\"chmod -R 755 {path}\")\n",
    "        \n",
    "        # Split the report text into manageable chunks\n",
    "        splits = better_chunking(report_text)\n",
    "        # Create metadata chunks for each text chunk\n",
    "        meta_data_chunks = chunk_metada(splits, ticker)\n",
    "        \n",
    "        # Initialize Google's embedding model\n",
    "        embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/embedding-001\",\n",
    "            google_api_key=os.getenv(\"google\")\n",
    "        )\n",
    "\n",
    "        # Create vectorstore with error handling\n",
    "        try:\n",
    "            # Attempt to create Chroma vector store from documents\n",
    "            vectorstore = Chroma.from_documents(\n",
    "                meta_data_chunks, \n",
    "                embeddings, \n",
    "                persist_directory=str(path)\n",
    "            )\n",
    "            \n",
    "            # Try to persist with retry logic\n",
    "            max_retries = 3\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    vectorstore.persist()\n",
    "                    print(f\"âœ… Vectors saved at: {path.resolve()}\")\n",
    "                    break\n",
    "                except Exception as persist_error:\n",
    "                    if attempt == max_retries - 1:\n",
    "                        raise persist_error\n",
    "                    print(f\"âš ï¸ Persist failed (attempt {attempt+1}), retrying...\")\n",
    "                    time.sleep(1)  # Wait before retrying\n",
    "            \n",
    "            return f\"Successfully ingested 10-K for {ticker}\"\n",
    "            \n",
    "        except Exception as chroma_error:\n",
    "            # Fallback: try with a different collection name\n",
    "            try:\n",
    "                collection_name = f\"{ticker}_{int(time.time())}\"\n",
    "                vectorstore = Chroma.from_documents(\n",
    "                    meta_data_chunks, \n",
    "                    embeddings, \n",
    "                    persist_directory=str(path),\n",
    "                    collection_name=collection_name\n",
    "                )\n",
    "                vectorstore.persist()\n",
    "                print(f\"âœ… Vectors saved with fallback collection: {path.resolve()}\")\n",
    "                return f\"Successfully ingested 10-K for {ticker} with fallback collection\"\n",
    "            except Exception as fallback_error:\n",
    "                # Try to fix permissions and retry\n",
    "                try:\n",
    "                    os.system(f\"chmod -R 755 {path}\")\n",
    "                    vectorstore.persist()\n",
    "                    print(f\"âœ… Vectors saved after fixing permissions: {path.resolve()}\")\n",
    "                    return f\"Successfully ingested 10-K for {ticker} after fixing permissions\"\n",
    "                except:\n",
    "                    raise fallback_error\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the ingestion process\n",
    "        error_msg = f\"An error occurred while ingesting the 10-K filing: {e}\"\n",
    "        print(error_msg)\n",
    "        \n",
    "        # Try to clean up on failure\n",
    "        try:\n",
    "            if path.exists():\n",
    "                shutil.rmtree(path)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        return error_msg\n",
    "\n",
    "@tool(description='asks quesstion from the indexed vector store')\n",
    "def query_data(ticker: str, query: str) -> str:\n",
    "    \"\"\"\n",
    "    Queries the vector store for a given company's and returns an answer to the query.\n",
    "\n",
    "    Args:\n",
    "        ticker (str): The stock ticker symbol of the company to query.\n",
    "        query (str): The user's question or query about the company.\n",
    "\n",
    "    Returns:\n",
    "        str: The answer generated by the model, or an error message if the query fails.\n",
    "\n",
    "    Process:\n",
    "        - Loads the vector store for the given ticker.\n",
    "        - Creates a hybrid retriever (vector + BM25) if possible.\n",
    "        - Enhances the user's query for better retrieval.\n",
    "        - Retrieves relevant documents and creates a context with citations.\n",
    "        - Formats a prompt for the model using the context and query.\n",
    "        - Invokes the model to generate an answer.\n",
    "        - Returns the model's answer or an error message.\n",
    "    \"\"\"\n",
    "    path = Path(\"INDEXED\") / ticker\n",
    "    if not path.exists():\n",
    "        return \"There's no such vectorstore yet.\"\n",
    "\n",
    "    try:\n",
    "        embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/embedding-001\",\n",
    "            google_api_key=os.getenv(\"google\")\n",
    "        )\n",
    "        vectorstore = Chroma(\n",
    "            persist_directory=str(path),\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "\n",
    "        # Hybrid retriever\n",
    "        retriever = create_hybrid_retreiver(vectorstore)\n",
    "\n",
    "        # Enhance query\n",
    "        enhanced = enhance_query(query,ticker)\n",
    "\n",
    "        #gets the top 5 filtered documents\n",
    "        # In query_data function, change this line:\n",
    "        filtered_docs = retriever.filter_documents(enhanced)\n",
    "        context = create_context_citations(filtered_docs)  # This now works correctly\n",
    "        prompt = PromptTemplate(\n",
    "            template=(\n",
    "                \"You are a senior and smart financial assistant. Use the following 10-K context \"\n",
    "                \"to answer the question clearly and accurately.\\n\\n\"\n",
    "                \"Context:\\n{context}\\n\\n\"\n",
    "                \"Question:\\n{query}\\n\\n\"\n",
    "                \"Answer with clear reasoning and cite sources.\"\n",
    "            ),\n",
    "            input_variables=[\"context\", \"query\"],\n",
    "        )\n",
    "\n",
    "        final_prompt = prompt.format(context=context, query=query)\n",
    "\n",
    "        # Generate answer\n",
    "        output = model.invoke(final_prompt)\n",
    "        new_output=answer_validator(enhanced_query=enhanced,answer=output.content,context=context,retriever=retriever)\n",
    "        return new_output\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred while querying the data: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce1f2c9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HeimdallState' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 21\u001b[0m\n\u001b[1;32m     13\u001b[0m stratergist_agent \u001b[38;5;241m=\u001b[39m create_react_agent(\n\u001b[1;32m     14\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     15\u001b[0m     tools\u001b[38;5;241m=\u001b[39m[get_current_date, get_current_markettrends],\n\u001b[1;32m     16\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrategist\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mstrategist_prompt\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#the brain of heimdall\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21morchestrator_node\u001b[39m(state:\u001b[43mHeimdallState\u001b[49m):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- ðŸ§  EXECUTING ORCHESTRATOR ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HeimdallState' is not defined"
     ]
    }
   ],
   "source": [
    "strategist_prompt = \"\"\"\n",
    "You are an expert financial strategist. Your job is to create a detailed mission plan for analyzing {ticker}.\n",
    "\n",
    "Consider the current market context and generate 3-5 key hypotheses to investigate:\n",
    "- Check recent earnings and financial health\n",
    "- Analyze market sentiment and news\n",
    "- Review technical indicators and price trends  \n",
    "\n",
    "Create a step-by-step plan assigning tasks to analysts.\n",
    "Output as a numbered list.\n",
    "\"\"\"\n",
    "\n",
    "stratergist_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[get_current_date, get_current_markettrends],\n",
    "    name='strategist',\n",
    "    prompt=strategist_prompt\n",
    ")\n",
    "\n",
    "#the brain of heimdall\n",
    "def orchestrator_node(state:HeimdallState):\n",
    "    print(\"--- ðŸ§  EXECUTING ORCHESTRATOR ---\")\n",
    "    try:\n",
    "        ticker=state['ticker']\n",
    "        if not ticker:\n",
    "            return {'error':'No ticker provided. Please provide a ticker to analyze.'}\n",
    "        message=state['messages']\n",
    "        mission_plan=stratergist_agent.invoke({'message':f'Create a mission plan for the investment analysis of {ticker} with the message {message}'})\n",
    "        state['mission_plan']=mission_plan['messages'][-1].content\n",
    "        return {'mission_plan':state['mission_plan'], \n",
    "        \"messages\": [AIMessage(content=f\"Mission plan created:\\n{mission_plan}\", name='Orchestrator')]\n",
    "  }\n",
    "    except Exception as e:\n",
    "        return {'error':f'An unexpected error occurred: {e}'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872d2486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def investment_strategist_node(state: HeimdallState) -> Dict:\n",
    "    \"\"\"\n",
    "    The final node in the graph. Synthesizes all analysis into a final report.\n",
    "    \"\"\"\n",
    "    print(\"--- ðŸ§  EXECUTING INVESTMENT STRATEGIST ---\")\n",
    "    \n",
    "    ticker = state['ticker']\n",
    "    financial_summary = state.get('financial_analysis', \"No data provided.\")\n",
    "    news_summary = state.get('news_analysis', \"No data provided.\")\n",
    "    technical_summary = state.get('technical_analysis', \"No data provided.\")\n",
    "\n",
    "    # The meta-prompt to guide the final synthesis\n",
    "    investment_strategist_prompt = f\"\"\"\n",
    "    You are a Senior Investment Portfolio Manager. You have received three reports from your junior analysts: a financial analysis from SEC filings, a market news and sentiment analysis, and a technical stock analysis.\n",
    "\n",
    "    Your task is to synthesize these findings into a final, professional investment thesis report.\n",
    "\n",
    "    Here are the reports from your team:\n",
    "\n",
    "    **1. Financial Analysis Report:**\n",
    "    {financial_summary}\n",
    "\n",
    "    **2. News & Sentiment Analysis Report:**\n",
    "    {news_summary}\n",
    "\n",
    "    **3. Technical Analysis Report:**\n",
    "    {technical_summary}\n",
    "\n",
    "    ---\n",
    "\n",
    "    **Your Final Report Structure:**\n",
    "\n",
    "    You must structure your final report in Markdown format with the following sections:\n",
    "\n",
    "    **## Investment Thesis for {ticker}**\n",
    "\n",
    "    **### 1. Executive Summary & Recommendation**\n",
    "    * Provide a brief, high-level overview of the investment thesis.\n",
    "    * **Crucially, you must conclude this section with a clear, one-word recommendation: `BUY`, `HOLD`, or `AVOID`.**\n",
    "\n",
    "    **### 2. Fundamental Analysis**\n",
    "    * Summarize the key findings from the financial (10-K) analysis. Discuss revenue, profitability, and any significant risk factors mentioned in the filing.\n",
    "\n",
    "    **### 3. Market Sentiment Analysis**\n",
    "    * Summarize the findings from the news analysis. Discuss the overall sentiment (Positive/Negative/Neutral) and the key news events driving that sentiment.\n",
    "\n",
    "    **### 4. Technical Analysis**\n",
    "    * Summarize the findings from the technical analysis. Discuss the stock's current price relative to its key moving averages and what the RSI indicates about its momentum.\n",
    "\n",
    "    **### 5. Final Justification**\n",
    "    * Provide a concluding paragraph that justifies your `BUY/HOLD/AVOID` recommendation by weighing the evidence from all three reports.\n",
    "\n",
    "    ---\n",
    "\n",
    "    Now, generate the complete investment thesis report for {ticker}.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the final report using the model\n",
    "    final_report = model.invoke(investment_strategist_prompt).content\n",
    "    \n",
    "    # Update the state with the final report\n",
    "    return {\n",
    "        \"final_report\": final_report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf4f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "handoff_to_quant_analyst_tool = create_handoff_tool(\n",
    "    agent_name=\"quantitative_analyst\",\n",
    "    description=\"Use this to delegate the task of performing quantitative analysis, such as calculating historical volatility and mean returns.\"\n",
    ")\n",
    "\n",
    "handoff_to_insider_agent_tool = create_handoff_tool(\n",
    "    agent_name=\"insider_agent\",\n",
    "    description=\"Use this to delegate the task of performing all insider trading analysis.\"\n",
    ")\n",
    "\n",
    "handoff_to_librarian=create_handoff_tool(\n",
    "    agent_name=\"librarian\",\n",
    "    description=\"Use this whenever u want to save any report or u need to ask questions from the data u saved into this financial report and make sure the data ingested is not small .\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1276ac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RiskSection(BaseModel):\n",
    "    summary: Annotated[str, Field(description=\"A concise, formal summary of the key risks in this category.\")]\n",
    "    main_threats: Annotated[List[str], Field(description=\"A prioritized list of the most significant threats identified.\")]\n",
    "    critical_risks: Annotated[List[str], Field(description=\"A list of the most critical and urgent risks requiring immediate attention.\")]\n",
    "    moderate_risks: Annotated[List[str], Field(description=\"A list of moderate risks that should be monitored but are less urgent.\")]\n",
    "    minor_risks: Annotated[List[str], Field(description=\"A list of minor or low-probability risks.\")]\n",
    "    overall_risk_level: Annotated[Literal['High', 'Medium', 'Low', 'Very Low'], Field(description=\"A formal assessment of the overall risk level for this category.\")]\n",
    "\n",
    "class FinancialRiskSection(RiskSection):\n",
    "    specific_risks: Annotated[List[str], Field(description=\"A detailed list of financial risks identified, with supporting evidence.\")]\n",
    "    rationale: Annotated[List[str], Field(description=\"A list of reasons and justifications for the financial risk assessment.\")]\n",
    "\n",
    "class NewsRiskSection(RiskSection):\n",
    "    specific_risks: Annotated[List[str], Field(description=\"A detailed list of news-related risks identified, with supporting evidence.\")]\n",
    "    rationale: Annotated[List[str], Field(description=\"A list of reasons and justifications for the news risk assessment.\")]\n",
    "\n",
    "class TechnicalRiskSection(RiskSection):\n",
    "    specific_risks: Annotated[List[str], Field(description=\"A detailed list of technical risks identified, with supporting evidence.\")]\n",
    "    rationale: Annotated[List[str], Field(description=\"A list of reasons and justifications for the technical risk assessment.\")]\n",
    "\n",
    "class FullRiskReport(BaseModel):\n",
    "    executive_summary: Annotated[str, Field(description=\"A formal executive summary of the overall risk profile, highlighting the most material risks across all categories.\")]\n",
    "    financial: FinancialRiskSection\n",
    "    news: NewsRiskSection\n",
    "    technical: TechnicalRiskSection\n",
    "    conclusion: Annotated[str, Field(description=\"A formal concluding statement summarizing the overall risk posture and any recommended actions or mitigations.\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb681feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a model for risk analysis\n",
    "\n",
    "#there will be 3 agents to analyze the risk of the company under another supervisor \n",
    "#1 one will check the financial statemtns of the company\n",
    "#2 one will check the news about the company\n",
    "#3 one will check the technical analysis of the company\n",
    "\n",
    "#and at the end it will give the risk analysis of the company and before making the final report it will be considered \n",
    "# --- THE RISK ANALYSIS GUILD ---\n",
    "\n",
    "# 1. Financial Risk Agent\n",
    "financial_risk_agent = create_react_agent(\n",
    "    model=model,\n",
    "    response_format=FinancialRiskSection,\n",
    "    tools=[get_latest_10k_filing, company_overview, get_cashflow,get_income_statements,get_balance_sheet,get_eearning,handoff_to_librarian], # It uses the same tools, but with a different goal\n",
    "    name='financial_risk_agent',\n",
    "    prompt='''You are a Financial Risk Analyst. Your primary responsibility is to thoroughly review the company\\'s 10-K filing, company overview, and analyze the latest cash flow and income statements using the provided tools. \n",
    "    Your goal is to identify and highlight all potential financial risks, vulnerabilities, and warning signs. \n",
    "\n",
    "    Pay particular attention to: \n",
    "    - Debt levels and maturity schedules\n",
    "    - Liquidity issues and cash flow constraints\n",
    "    - Unusual or increasing liabilities\n",
    "    - Negative trends in revenue, profit, or margins\n",
    "    - Stated risk factors in the filings\n",
    "    - Any indications of financial instability or weakness\n",
    "    - you can ingest important data using librarian agent \n",
    "    - you can ask questions to librarian agent to retrieve from the library of document stored\n",
    "\n",
    "    Completely ignore positive or neutral information. Focus exclusively on negative aspects and red flags. \n",
    "    Provide a concise, bullet-point summary of the top 3-5 most significant financial risks, each with a brief explanation based on your findings. Be specific and use evidence from the documents and data provided.''',\n",
    ")\n",
    "\n",
    "# 2. News Risk Agent\n",
    "news_risk_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[search_web, analyze_news_sentiment],\n",
    "    response_format=NewsRiskSection,\n",
    "    name='news_risk_agent',\n",
    "    prompt='''You are a Market Risk Analyst specializing in news. Your primary responsibility is to thoroughly search for and analyze the most recent news about a company using the provided tools.\n",
    "    Your goal is to identify and highlight all potential news-related risks, controversies, and negative developments that could impact the companyâ€™s reputation, operations, or stock price.\n",
    "    Pay particular attention to:\n",
    "    - Negative headlines and adverse media coverage\n",
    "    - Scandals, fraud allegations, or executive misconduct\n",
    "    - Regulatory investigations, fines, or compliance issues\n",
    "    - Lawsuits, legal disputes, or class actions\n",
    "    - Signs of declining market sentiment or public perception\n",
    "\n",
    "    Completely ignore positive or neutral news. Focus exclusively on negative aspects and red flags.\n",
    "    Provide a concise, bullet-point summary of the top 3-5 most significant news-related risks, each with a brief explanation based on your findings. Be specific and reference the news sources or headlines where possible.'''\n",
    ") \n",
    "\n",
    "# 3. Technical Risk Agent\n",
    "technical_risk_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[get_technichal_analysis,get_current_markettrends,get_market_status,handoff_to_librarian],\n",
    "    response_format=TechnicalRiskSection,\n",
    "    name='technical_risk_agent',\n",
    "    prompt='''You are a Technical Risk Analyst. Your primary responsibility is to thoroughly examine a stock\\'s chart data and identify all significant bearish (negative) technical signals that could indicate increased risk or potential for price decline.\n",
    "    Carefully analyze the following:\n",
    "    - Overall trend direction (identify clear downtrends or trend reversals)\n",
    "    - Bearish moving average crossovers (such as the \"death cross\" where the 50-day MA crosses below the 200-day MA)\n",
    "    - Overbought RSI (Relative Strength Index) readings or bearish RSI divergences\n",
    "    - Breakdown of key support levels or formation of bearish chart patterns (e.g., head and shoulders, double top)\n",
    "    - Unusual spikes in trading volume accompanying price drops\n",
    "\n",
    "    Ignore neutral or bullish signals. Focus exclusively on negative technical indicators and red flags.\n",
    "    Provide a concise, bullet-point summary of the top 2-3 most significant technical risks, each with a brief explanation based on your analysis. Be specific and reference the technical evidence you observe.'''\n",
    ")\n",
    "# The supervisor for the risk guild\n",
    "risk_supervisor = create_supervisor(\n",
    "    model=model,\n",
    "    agents=[financial_risk_agent, news_risk_agent, technical_risk_agent],\n",
    "    response_format=FullRiskReport,\n",
    "    prompt='''You are the Chief Risk Officer overseeing a team of specialist risk analysts. Your mandate is to deliver a comprehensive, high-level risk assessment of a company, integrating insights from financial, news, and technical domains.\n",
    "\n",
    "    Your responsibilities:\n",
    "    - Strategically delegate the analysis of financial, news, and technical risks to your respective expert agents.\n",
    "    - Critically review and synthesize the findings from each agent, ensuring that all significant red flags and risk factors are captured.\n",
    "    - Integrate the agentsâ€™ reports into a single, cohesive \"Risk Report\" that clearly communicates the most material risks, their potential impact, and any interdependencies or compounding effects.\n",
    "    - Prioritize clarity, conciseness, and actionable insight for senior decision-makers.\n",
    "    - Do not include positive or neutral information; focus exclusively on negative aspects and risk exposures.\n",
    "\n",
    "    After completing your synthesis, respond with the word \"FINISH\".''',\n",
    "    add_handoff_back_messages=True,\n",
    "    output_mode='full_history'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467ca4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manager 1: Financial Analyst Manager\n",
    "financial_analyst = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[get_latest_10k_filing, company_overview, get_cashflow, get_eearning, get_income_statements,handoff_to_librarian],\n",
    "    name='financial_analyst',\n",
    "    prompt='''You are a Senior Financial Analyst tasked with delivering a high-quality, actionable financial analysis for a company. Your responsibilities include:\n",
    "\n",
    "1. Use the `get_company_overview` tool first to obtain a concise summary of the companys business model, sector, and key metrics.\n",
    "2. Fetch and thoroughly analyze the latest 10-K filing to identify:\n",
    "   - Key financial metrics (revenue, net income, cash flow, debt, margins, etc.)\n",
    "   - Major risk factors and uncertainties disclosed by the company\n",
    "   - Notable trends or changes compared to previous filings\n",
    "   - Any red flags or warning signs in the financial statements\n",
    "3. Use the available tools to extract and cross-verify data on cash flow, earnings, and income statements.\n",
    "4. Synthesize your findings into a structured, bullet-point summary that highlights:\n",
    "   - The companyâ€™s current financial health and stability\n",
    "   - The most significant financial risks or weaknesses\n",
    "   - Any material changes or unusual items that require attention\n",
    "5. Be thorough, objective, and concise. Focus on what matters most for senior decision-makers. Avoid unnecessary detail or positive spin; emphasize risks and material issues.\n",
    "6. Reference specific data points, figures, or sections from filings where possible to support your analysis.\n",
    "\n",
    "Your output should enable a senior analyst to quickly understand the companys financial position and the most important risks or concerns requiring further review.''',\n",
    ")\n",
    "\n",
    "\n",
    "# Manager 2: News Analyst Manager\n",
    "news_analyst = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[search_web, analyze_news_sentiment, handoff_to_insider_agent_tool],\n",
    "    name='news_analyst',\n",
    "    prompt='''You are a professional News Analyst specializing in synthesizing market sentiment and news-driven risk factors for equities. Your responsibilities include:\n",
    "\n",
    "1. Conduct comprehensive searches for the most recent and relevant news regarding the specified ticker, prioritizing reputable financial sources.\n",
    "2. Critically analyze the sentiment of news articles, identifying key drivers, themes, and any emerging risks or controversies that could materially impact the company.\n",
    "3. Quantitatively assess and report the overall market sentiment, providing a clear sentiment score and a concise, structured news summary.\n",
    "4. When appropriate, leverage the Insider Agent to incorporate insider trading activity and sentiment as additional context for your analysis.\n",
    "5. Integrate insights from insider transactions and sentiment to enhance the depth and accuracy of your final output.\n",
    "6. Maintain a high standard of objectivity, data-driven reasoning, and professional clarity. Reference specific news items, dates, and sources where possible.\n",
    "\n",
    "Your output should enable senior decision-makers to quickly understand the most significant news-driven risks and market perceptions affecting the company. Focus on material risks, negative developments, and actionable intelligence. Avoid speculation and ensure all claims are supported by evidence.'''\n",
    ")\n",
    "\n",
    "# Manager 3: Technical Analyst Manager\n",
    "technical_analyst_manager = create_react_agent(\n",
    "    model=model,\n",
    "    name='technical_analyst',\n",
    "    tools=[get_technichal_analysis, handoff_to_quant_analyst_tool,handoff_to_librarian],\n",
    "    prompt='''You are a professional Technical Analyst responsible for delivering high-quality, actionable technical analysis of equity securities. Your responsibilities include:\n",
    "\n",
    "1. Rigorously analyze stock price data and a comprehensive set of technical indicators (e.g., moving averages, RSI, MACD, volume trends).\n",
    "2. Identify and clearly articulate prevailing trends, key support and resistance levels, momentum shifts, and any technical patterns that may signal significant price movements.\n",
    "3. Provide a structured technical outlook, including a confidence level and rationale for your assessment.\n",
    "4. When appropriate, collaborate with the Quantitative Analyst to supplement your analysis with advanced quantitative techniques.\n",
    "5. Ensure your analysis is objective, evidence-based, and professionally formatted. Reference specific data points, timeframes, and indicator values to support your conclusions.\n",
    "\n",
    "Your output should empower portfolio managers and senior analysts to make informed decisions based on the most relevant technical signals and risk factors. Focus on clarity, conciseness, and actionable insights.'''\n",
    ")\n",
    "\n",
    "# 2. Senior Supervisor Agent (Portfolio Manager)\n",
    "supervisor_graph = create_supervisor(\n",
    "    model=model,\n",
    "    agents=[\n",
    "        financial_analyst,\n",
    "        news_analyst, \n",
    "        technical_analyst_manager\n",
    "    ],\n",
    "    prompt='''You are a professional Senior Investment Portfolio Manager overseeing a team of specialized analysts. Your mandate is to deliver a comprehensive, high-level investment analysis and risk assessment for a given company.\n",
    "\n",
    "You supervise three professional analysts:\n",
    "1. financial_analyst â€“ provides in-depth analysis of SEC filings and company fundamentals.\n",
    "2. news_analyst â€“ synthesizes market sentiment and news-driven risk factors.\n",
    "3. technical_analyst â€“ delivers actionable technical analysis of stock price data and indicators.\n",
    "\n",
    "Your workflow:\n",
    "1. Strategically delegate tasks to the appropriate analyst based on the mission plan and the unique strengths of each team member.\n",
    "2. Critically review and synthesize the analysts' work, ensuring all material risks, red flags, and actionable insights are captured.\n",
    "3. Integrate the findings into a single, cohesive report that prioritizes clarity, conciseness, and professional rigor for senior decision-makers.\n",
    "4. When all analysis is complete and the report is ready, respond with \"FINISH\".\n",
    "\n",
    "Assign work to one analyst at a time, ensuring thoroughness and quality at each step. Do not call agents in parallel. Maintain a high standard of professionalism, objectivity, and actionable intelligence throughout the process. Focus exclusively on risks, negative developments, and material issues requiring attention.'''\n",
    "    ,\n",
    "    add_handoff_back_messages=True,\n",
    "    output_mode=\"full_history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e139d868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The specialist agent for quantitative analysis\n",
    "quantitative_analyst = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[advaced_analyst],\n",
    "    name='quantitative_analyst',\n",
    "    prompt='''You are a specialist  Quantitative Analyst. Your only job is to use your tool\n",
    "    to perform a standard quantitative analysis on a stock's performance over a 6-month period.\n",
    "\n",
    "    You MUST call the `get_analytics_sliding_window` tool with the `range_str` parameter set to \"6month\".\n",
    "    \n",
    "    After getting the data, provide a concise summary of the mean return and the annualized standard deviation (volatility)\n",
    "    and .'''\n",
    ")\n",
    "# --- Create the New Specialist Insider Agent ---\n",
    "insider_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[get_insider_info, get_insiders_sentiment], # Note: get_current_date isn't needed if the tools do it internally\n",
    "    name='insider_agent',\n",
    "    prompt='''You are a specialist Insider Trading Analyst. Your job is to provide a concise summary of insider sentiment and recent trading activity for a given company.\n",
    "\n",
    "    **Your Workflow:**\n",
    "    1.  First, use the `get_insider_sentiment` tool to get the high-level sentiment score (Positive/Negative/Neutral).\n",
    "    2.  Next, use the `get_insider_info` tool to get the list of the most recent raw buy/sell transactions.\n",
    "    3.  Finally, synthesize the information from both tools into a single, concise summary. State the overall sentiment and then list the top 3-5 most significant recent transactions as evidence.\n",
    "    '''\n",
    ")\n",
    "\n",
    "\n",
    "# This agent is the sole keeper of the Corporate Library.\n",
    "librarian_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[ingest_10k_filling, query_data],\n",
    "    name='librarian_agent',\n",
    "    prompt='''You are the Master Librarian for the Heimdall system. Your sole responsibility is to manage the Corporate Library of indexed financial documents.\n",
    "\n",
    "    You have two primary tasks:\n",
    "    1.  **Ingest**: When given a report, you MUST use the `ingest_10k_to_library` tool to save it to the archives.\n",
    "    2.  **Query**: When asked a specific question, you MUST use the `query_data` tool to find the answer within the archives.\n",
    "\n",
    "    You do not interpret the information. You only ingest, find, and retrieve it.\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33974c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00542d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactCheckResult(BaseModel):\n",
    "    \"\"\"Individual fact check result\"\"\"\n",
    "    claim: str = Field(description=\"The specific claim being fact-checked\")\n",
    "    is_accurate: bool = Field(description=\"Whether the claim is factually accurate\")\n",
    "    confidence_score: float = Field(ge=0.0, le=1.0, description=\"Confidence in the fact-check (0-1)\")\n",
    "    supporting_sources: List[str] = Field(description=\"Sources that support or contradict the claim\")\n",
    "    contradicting_sources: List[str] = Field(description=\"Sources that contradict the claim\")\n",
    "    verification_method: str = Field(description=\"How this fact was verified\")\n",
    "    last_verified: datetime = Field(description=\"When this fact was last verified\")\n",
    "class DataConsistencyCheck(BaseModel):\n",
    "    \"\"\"Check for data consistency across sources\"\"\"\n",
    "    metric_name: str = Field(description=\"Name of the financial metric being checked\")\n",
    "    primary_value: Optional[float] = Field(description=\"Value from primary source\")\n",
    "    secondary_value: Optional[float] = Field(description=\"Value from secondary source\")\n",
    "    variance_percentage: Optional[float] = Field(description=\"Percentage difference between sources\")\n",
    "    is_consistent: bool = Field(description=\"Whether values are within acceptable variance\")\n",
    "    acceptable_variance: float = Field(default=5.0, description=\"Acceptable variance percentage\")\n",
    "\n",
    "class SourceReliability(BaseModel):\n",
    "    \"\"\"Assessment of source reliability\"\"\"\n",
    "    source_name: str = Field(description=\"Name of the data source\")\n",
    "    reliability_score: float = Field(ge=0.0, le=1.0, description=\"Reliability score (0-1)\")\n",
    "    last_updated: Optional[datetime] = Field(description=\"When source was last updated\")\n",
    "    data_freshness: str = Field(description=\"How fresh the data is (real-time, daily, etc.)\")\n",
    "    known_issues: List[str] = Field(description=\"Known issues with this source\")\n",
    "\n",
    "class ComprehensiveFactCheck(BaseModel):\n",
    "    \"\"\"Complete fact-checking report\"\"\"\n",
    "    ticker: str = Field(description=\"Stock ticker being analyzed\")\n",
    "    fact_check_results: List[FactCheckResult] = Field(description=\"Individual fact check results\")\n",
    "    data_consistency_checks: List[DataConsistencyCheck] = Field(description=\"Cross-source data consistency\")\n",
    "    source_reliability: List[SourceReliability] = Field(description=\"Source reliability assessments\")\n",
    "    overall_accuracy_score: float = Field(ge=0.0, le=1.0, description=\"Overall accuracy score\")\n",
    "    critical_issues: List[str] = Field(description=\"Critical accuracy issues found\")\n",
    "    recommendations: List[str] = Field(description=\"Recommendations for improving accuracy\")\n",
    "    fact_check_timestamp: datetime = Field(default_factory=datetime.now)\n",
    "\n",
    "structured_llm = model.with_structured_output(FactCheckResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67b644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_report(report:str):\n",
    "    ''' the use of this method is to evaluate the report based on the report strucutre and give the output'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612d07e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_checker=create_react_agent(\n",
    "    model=structured_llm,\n",
    "    tools=[ query_data, search_web, handoff_to_insider_agent_tool,get_current_markettrends],\n",
    "    name='fact_checker',\n",
    "    prompt='''\n",
    "You are the Fact Checker for the Heimdall system. Your primary responsibility is to verify the accuracy and reliability of information provided by other agents or found in external sources.\n",
    "\n",
    "**Your Workflow:**\n",
    "1. When given a claim, statement, or report, you must use the available tools to independently verify the facts.\n",
    "    - Use `query_data` to check the Corporate Library for relevant documents or evidence.\n",
    "    - Use `search_web` to find up-to-date information from reputable online sources, such as news articles, press releases, or official company websites.\n",
    "    - Use `get_market_status` to confirm the current status of the financial markets if relevant to the claim.\n",
    "2. For each fact you check, clearly state whether it is supported, contradicted, or unverified based on the evidence you find.\n",
    "3. If you find conflicting information, present both sides and explain the discrepancy.\n",
    "4. If you cannot verify a claim, state explicitly that it could not be verified and suggest possible next steps or sources for further investigation.\n",
    "\n",
    "**Output Format:**\n",
    "- For each claim or statement, provide:\n",
    "    - The original claim.\n",
    "    - The sources you checked (tool and result).\n",
    "    - Your conclusion (Supported / Contradicted / Unverified).\n",
    "    - A brief explanation of your reasoning.\n",
    "\n",
    "Be thorough, impartial, and concise. Do not speculate or provide opinionsâ€”only report on what you can verify with the available tools and data.\n",
    "    '''\n",
    ")\n",
    "\n",
    "\n",
    "evaluator_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[query_data, search_web,handoff_to_insider_agent_tool,get_cashflow],\n",
    "    name='evaluator',\n",
    "    prompt=\"\"\"\n",
    "You are the Evaluator for the Heimdall system. Your primary responsibility is to critically assess the quality, accuracy, and completeness of the reports and analyses produced by other agents.\n",
    "\n",
    "**Your Workflow:**\n",
    "1. Carefully review the provided report or analysis.\n",
    "2. Check for factual accuracy, logical consistency, and completeness of the information.\n",
    "3. Use the available tools to independently verify any claims, data points, or conclusions that seem questionable or require further evidence.\n",
    "    - Use `query_data` to access internal documents or data.\n",
    "    - Use `search_web` to find up-to-date information from reputable online sources.\n",
    "    - Use `get_market_status` to confirm the current status of the financial markets if relevant.\n",
    "4. Identify any errors, omissions, or unsupported assertions.\n",
    "5. Provide constructive feedback, highlighting strengths and weaknesses, and suggest specific improvements if needed.\n",
    "\n",
    "**Output Format:**\n",
    "- A summary of your evaluation, including:\n",
    "    - Key strengths of the report.\n",
    "    - Any issues found (with evidence or reasoning).\n",
    "    - Suggestions for improvement.\n",
    "    - A final verdict: Acceptable / Needs Revision / Unacceptable.\n",
    "\n",
    "Be objective, thorough, and concise. Do not add new analysisâ€”focus on evaluating what is presented.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb0657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this as your ONLY state definition\n",
    "class HeimdallState(TypedDict):\n",
    "    ticker: str\n",
    "    mission_plan: Optional[str]\n",
    "    # This will store the final reports from each manager\n",
    "    financial_report: Optional[str]\n",
    "    news_report: Optional[str]\n",
    "    technical_report: Optional[str]\n",
    "    # The final, synthesized report\n",
    "    research_report:Optional[str]\n",
    "    risk_report:Optional[str]\n",
    "    final_report: Optional[str]\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43608b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_agent(agent,agent_name:str,state:HeimdallState,key_to_update:str):\n",
    "    result=agent.invoke(state['messages'])\n",
    "    return {key_to_update:result.content, \"messages\": state[\"messages\"] + [AIMessage(content=str(result), name=agent_name)]}\n",
    "\n",
    "\n",
    "def research_supervisor(state: HeimdallState):\n",
    "    return run_agent(supervisor_graph,'supervisor',state,'research_report')\n",
    "\n",
    "def risk_agent(state: HeimdallState):\n",
    "    return run_agent(risk_supervisor,'risk_analyst',state,'risk_report')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea0984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_reports_node(state: HeimdallState) -> Dict:\n",
    "    \"\"\"\n",
    "    This node takes the research and risk reports and synthesizes them\n",
    "    into a single, cohesive summary for the final strategist.\n",
    "    \"\"\"\n",
    "    print(\"--- âœï¸ HEIMDALL.EDITOR: Synthesizing Research and Risk reports ---\")\n",
    "    \n",
    "    research_report = state.get(\"research_report\", \"No research report was generated.\")\n",
    "    risk_report = state.get(\"risk_report\", \"No risk report was generated.\")\n",
    "\n",
    "    # A prompt to guide the synthesis\n",
    "    joiner_prompt = f\"\"\"\n",
    "    You are a Chief Editor. You have been given two reports: a primary research report and a detailed risk analysis.\n",
    "    Your job is to synthesize these into a single, cohesive, and balanced summary.\n",
    "\n",
    "    Do not leave anything important out, but be concise. The final output should be a single, unified report.\n",
    "\n",
    "    **Primary Research Report:**\n",
    "    ---\n",
    "    {research_report}\n",
    "    ---\n",
    "\n",
    "    **Risk Analysis Report:**\n",
    "    ---\n",
    "    {risk_report}\n",
    "    ---\n",
    "\n",
    "    **Your Synthesized Report:**\n",
    "    \"\"\"\n",
    "    \n",
    "    # Invoke the model to create the joined report\n",
    "    joined_report = model.invoke(joiner_prompt).content\n",
    "    \n",
    "    # We'll save this to a new field in our state\n",
    "    return {\"final_report\": joined_report}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fb83e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found edge starting at unknown node 'risk_analyst'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m workflow\u001b[38;5;241m.\u001b[39madd_edge(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoin_reports\u001b[39m\u001b[38;5;124m\"\u001b[39m, END)  \u001b[38;5;66;03m# Joiner â†’ End  \u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Compile\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mworkflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/newenv/lib/python3.10/site-packages/langgraph/graph/state.py:829\u001b[0m, in \u001b[0;36mStateGraph.compile\u001b[0;34m(self, checkpointer, cache, store, interrupt_before, interrupt_after, debug, name)\u001b[0m\n\u001b[1;32m    826\u001b[0m interrupt_after \u001b[38;5;241m=\u001b[39m interrupt_after \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    828\u001b[0m \u001b[38;5;66;03m# validate the graph\u001b[39;00m\n\u001b[0;32m--> 829\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    833\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;66;03m# prepare output channels\u001b[39;00m\n\u001b[1;32m    838\u001b[0m output_channels \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__root__\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschemas[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_schema]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    846\u001b[0m     ]\n\u001b[1;32m    847\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/newenv/lib/python3.10/site-packages/langgraph/graph/state.py:756\u001b[0m, in \u001b[0;36mStateGraph.validate\u001b[0;34m(self, interrupt)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m source \u001b[38;5;129;01min\u001b[39;00m all_sources:\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes \u001b[38;5;129;01mand\u001b[39;00m source \u001b[38;5;241m!=\u001b[39m START:\n\u001b[0;32m--> 756\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound edge starting at unknown node \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m START \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_sources:\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    760\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraph must have an entrypoint: add at least one edge from START to another node\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    761\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found edge starting at unknown node 'risk_analyst'"
     ]
    }
   ],
   "source": [
    "\n",
    "workflow = StateGraph(HeimdallState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"orchestrator\", orchestrator_node)  \n",
    "workflow.add_node(\"supervisor\", research_supervisor)  # Use the compiled supervisor\n",
    "workflow.add_node(\"investment_strategist\", investment_strategist_node)\n",
    "workflow.add_node('risk analyst',risk_agent)\n",
    "workflow.add_node(\"join_reports\", join_reports_node)  # Add the new node\n",
    "\n",
    "# Set up the flow correctly\n",
    "workflow.set_entry_point(\"orchestrator\")  # Start with orchestrator\n",
    "workflow.add_edge(\"orchestrator\", \"supervisor\")  # Orchestrator â†’ Supervisor  \n",
    "workflow.add_edge(\"supervisor\", \"investment_strategist\")  # Supervisor â†’ Final Report\n",
    "workflow.add_edge(\"investment_strategist\", END)  # End\n",
    "workflow.add_edge('orchestrator','risk_analyst')\n",
    "workflow.add_edge('risk_analyst','investment_strategist')\n",
    "workflow.add_edge(\"investment_strategist\", \"join_reports\")  # Final Report â†’ Joiner\n",
    "workflow.add_edge(\"join_reports\", END)  # Joiner â†’ End  \n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba68057b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMUAAAGwCAIAAACWwjZhAAAQAElEQVR4nOydB1wUxxfHZ6/SqxQVAVGxoWJEE42xK7HX2Gussfxji0YTI2oSe0k00RjTbLG3qIlG0diisaFiRxCwYAMpR7nj9v7vWDwPuDvm9M7bhfcV77M3O9ve/u7Nm9nZGYlGoyEIYiEkBEEsB+oJsSSoJ8SSoJ4QS4J6QiwJ6gmxJCVET6ps1fnIlKR4ZXammlUTVU6BRhBGRDQsfIo0LJufwhCunYQRMxq1Rj9FtwlDGJYtsB+JiIHtCyWKxASOCLmJXrJIlL/tywWJiM1lC+xNDis1UrmojJ+01ruunj72RPgwQm9/2rE88XFiTq6KyOwYmZ1IIoObRHKVBfK80BOj0Unhxe0XiRlWrdFP0W0CKRp1wf2ItZ+FEkViDatmCm2u+8odWptNQtjcAhtK5CA1NjuLVWayoEjI6e4tbTPQu0xZAQtLwHraMC8+5ZHKwVlUuY5Tk+7eROCcOfDs2r9pilS1vbNo8OcBYpmYCBBB6unUvicXI1Ndy0h6jPezsy9pIeCWJQmPE5X+1e07jShPhIbw9LR5ScLzx6oOI3zLBzmSksvqaTFSuXhIREUiKASmp8hNSQk3MwfPDCKlgM1L43NzSL9PA4hwEJKeNsy9q8xhh0SUCjFxbFmakPpUNfyrSkQgiIhA2LPqXk62ulSJCeg5wd+1jHTdV3eJQBCGnhJuZdyLyf5wlmB+phYEJJWVoT6y9RERAsLQ0/6fkmo2ciallXbDfa+dTidCQAB6itz6CGK8pt18SGnFL8jRwVkMFVvCewSgp9vn06vWcyKlm+YfeD69ryS8h+96uns1Q5VDWvTyJaWbwJouEilzeDPfoyi+6+ncoRRH1zd9klu2bJk5cyYxn08//XT37t3EOpQpL4+/piD8hu96ev5U5RtoR94s165dI6/EK29IQ9Uwx5xMlvAbvutJmc0G1nAg1uHu3bvgUVq3bt2qVauJEydGRUVB4ogRI/bu3btv376wsLAbN25AyubNm8eOHdusWbPw8PBp06bdu3eP23zTpk2QcvTo0QYNGixatAjyP3jwYM6cOZCTWIGQhu6smigzeB1F8V1PGjWpaB09KZVKkI5YLF6+fPnKlSslEsmECROys7NXr14dEhLSvn37c+fOVatWDUS2cOHCOnXqgGJmzZqVnJz8+eefc3uQyWQKhWLbtm2zZ8/u2bPnyZMnIXHGjBmgMGIdRGISez2L8BheP5yHdjzCEDsnGbEC8fHxII4+ffqAaODrvHnzLly4kJubWyhbrVq1IJzy9/cHwcFXlUoFsktNTXV1dWUYBvQ3aNCg+vXrw6qcnBxiZRixKD1ZTXgMr/Wk7f2mYYh1AIm4u7tHRES0a9euXr164IGgwCqaDRwYFHCLFy+Ojo4Gb8QlghBBT9xyzZo1yRsDLGI1g1gEXpd3js5isF9urlV+kXK5/Mcff2zcuPHGjRuHDh3apUuX/fv3F832zz//QGhVo0YNyHz27NkVK1YUygClHnlTsKzGzoXwGb7HTyIRibtqrUpyYGDg+PHjIfpesmRJ5cqVv/jiCy4A12fnzp2hoaFjxowJDg6GAi493ZbPPSAer1D1Tdd2zYLvepLImLjoTGIFoHK3Z88eWLCzs2vSpMn8+fMhQrp+/XqhbBAqeXu/7EwcGRlJbMTtqOfw6ebJ697lfNeTg4vk3i2r1GhAKFAvW7ZsWWJiIsTmv/zyCwTjEEXBqgoVKkC0BKUbxEnglk6fPg11PVi7YcMGbtuHDx8W3SEUoKA8XWZiaa6eTpfx/k0FvusptIlbZrpV4ieQzvTp0//888+uXbt279794sWLq1atCgrS9q/q1q0bFG1Qxt2+fXv06NGNGjWCEKphw4ZJSUnQZACx1P/+97+//vqr6D4//PBDUOGkSZOysiz/G3h4J6dsRV4XdkQQ/TO/mxRTr5X7O209SSkmNVm5bk7C2KWVCb8RQP+CCsH2UUeek9LN3tUPXTwF8CaPAE6x08jy302MuXT8WZ33DLuocePGXblyxeAqiGO4dsiiQMuTlR6MAMb2rFaroUAwdkqHDh0ytirlkYr/zokI5X2EM38+O38oZfRiwwbNzMyE+2RwlQk92dvbG1v1+phoVjBxSs7Ohvug/vhZjKevvNu4CoT3COb9lvVz4xlGYC8PWYS/fnuYeDNz+NfC6DsvmPdb+k8LyFaot69IJKWJ/w48jotWCEVMRHDvc25cEC8Sa3pPCiSlgCPbkm6dU4ycJ6S3eoT3vvlPM2MlYmbQFwJ7EdtcNi1KeP5YOWqBAGJwfQQ5HsaOFYkP43Iq1rRv96HwRowoln+2J139N8PZQzJgeiARGkIdr+fBHcW+NQ+VSuJVQfZeZ4+yFQX/AkzGc+XfGx7dv5NDNKRxF/fQpoJsvxX2eGLXTqec+StZkaphxMTeUezkLnZwFstkolx1gU5C3Nhz+iN+6Y9Gl59eaDyx/C1I0f0Y3txQNmNH5xCLSK6KzcpQZzzPVaSpNSyRyElII+fGnQT8pqHgx6fjuBCZfPeaIi0lV61iNSxTaLxDDngkp7vYQrfcAAXvvyZvSzHsociuisXYsSQyImIYsYxxdJWUC5K/21HwQ6KREqMna7N58+b4+PgpU6YQxCQ4vi8VJhq1EX3QRlSgnihBG1GBeqIEbUSFSqWSSqUEKQ7BPL+zLeifKEEbUYF6ogRtRAXqiRK0ERUYP1GCeqIC/RMlaCMqUE+UoI2oQD1RgjaiAvVECdqICozHKUE9UYH+iRK0ERWoJ0rQRlSo1WrUEw1oIyogfkI90YA2ogLLO0rQRlSgnihBG1GBeqIEbUQF6okStBEV2J5JCeqJCvRPlKCNqPD19RWJsG908aCeqHj8+LE1hoAueaCeqIDCDvVEA+qJCtQTJagnKlBPlKCeqEA9UYJ6ogL1RAnqiQrUEyWoJypAT8aGzEf0QT1RIRaL0T/RgHqiAss7SlBPVKCeKEE9UYF6ogT1RAXqiRLUExWoJ0pQT1SgnihBPVEB7QXY/kQD6okK9E+U4PwIpggPD3/y5AmYiGEYkUjEsiwsBwYG7ty5kyCGwD6spgA9gYygsOM6+8KnTCbr06cPQYyAejJFv379/P399VPga6dOnQhiBNSTKXx8fNq0aaP7CqVe27Zt7ezsCGIE1FMx9O/fX+ei/Pz8unfvThDjoJ6KwcnJqUuXLhBCwXLr1q1dXV0JYhxe1O9uRaXGX89S5egSNOTF3JgihrAFTzB/usu89RpDq17sQyMSMeyL6TSLZhYxGlbzYgbOvNkTGe1/pqg9oFp35swZllWHhdWTy+2J8SkSxWJGrdYYnJ9T/6J0l/Bitk9N3o0wfC3cxI5G9pmPTKrxrCCr28T2U8TaWE/QSPjLzDiVEhp4RCqloTMBB8rqf9fkzY2ZJxGwaaFVWo3kf8nLoJVM3jKjvVC24I7FhNW1UL64adrbZNge2nSROH//BgWq22eRe59nZL0JP7VCBy2LiG5vkIct+Lt5uZO8DblNXq4V5V8ah8yOUSlZqIO2H+brV9mWUynbUk8gptWfxgWG2DfuUgKnKX/zXDn1LCoypfMo3/KVbCYpW+pp5dSYem3cq4cJciJv3rJ2Tszwr/2hnYzYApvF4wfWPZBIGRSTxXHxFG//5gGxETbT05NEpYsHjoBjeXwDHDJSbPao0WZ6ysl6Wd9BLIjcXqJS2cywNutfAPUgFh/YWwOIiHNtFhNjfxXEkqCeShw2jSNspydGk9cujFgakS2jUtvpSQNNXxiPWwGb/kixvCt5aGwoKduWd+ifrANTCut3Guy6bhXynjyXvvYnhsHmTKvAlNbyzrYVEcQq2ExPUNaxWNxZAe2PVFz6yjutT0Y9WQGtUdU2s2zp6j/+1defj/t4KCnRaONx2wUSNtMTI9L+CZqdu7bMnT+TmE/X7q0fPLxPrEMpjcc1bKHe38Lj5s1rxHySkh4+f55CrIamlD6/eyXWrltz4ODep08fe3v7htapN2H8NJFIFBsbM3R477lfLVu05Es3N/c1q3+HnP/+e/yb5fOfPHlcuVJwly49276f/1KvVCKNijr/1dzP4abCqnHjptSoHkLyZiT76efvT5858fhxUkhIaNfOPd95pzG3SULC3V9+XRV16Tw0mNWsWbt3z4G1aoWOnzji0qULsPbgwX0/rFp/5UrUxt9/gfOZGTEFDjduzGQ4gcgjBy5fuZiWllq9WsiAAcPqhoZdjDo3cdIo2Kpf/87vvtv0y9mLjV0UpHfu2nJg/2HHTkTeu5ewfesBShMxNq0227DI0Zh75XBTd+3e8tHI8du2Hhj64eij//y9ddsGSOdmOly7fk2vngMmTfyc5IlpxszJQz8cM2/ut40bN1+wcPahw39xO3n0OGnPH9umT5sDq5Qq5cJFs7lm1W+XL9i2fWPXLr02bvijaZOWM2dN+efYYUhXKpUgHbFYPH/e8sULV0rEks8+n5Cdnb1syerq1UPatGl/5PC54CrVZDJZZqZiz55t0z6dDVqEDCDZnJycT6fO+vqrZf7+gbBVcvIzkBToHna7Yf1uTkzGLoq7rr37d1auXPWLz+cSajSl8/kdY+wdNiOkZ6T/vum3j0ZNaNy4GXxt1rRVbOzt9Rt+6ta1N/fcpn7YOx/06MdlhpvU5L0WrVu15dIVigy42dyqJ08erVq5ztnJGZZh20WLvwT/YWdnDx6ib5/BnTpqX/9t17ZzdPSltet+BGElJsanpCR379YHRAOrZn4x79LlC0XH7oFzAA317j3orbr1uZQ1qzfZ29u7urrBMvin3Xu2XYmOgh1SXhSICfbp4uIKro6YQ2mNn8y8arivKpWqel7ZxBEcXD0jI+P+/URu5szgKtW5dJZl78TebpUnJo5RIz/WLVeqFMyJCXB10d5s0AGUaOCH6oc11GWDcufPv/akpqX6+flDGTpvQUTrVu0gMSSkDvgYYydZrWpN3TIoeM1PK6CUfPbsKZdSNGwycVGBgUHwtWpwDSIobPm8xazSLjlZe1fs5C/HorC3d4DPrKxMZ2cXWJDJ5Vw66AMkJZcbHrVCf9pW3QPpjIx0+CzalJCS/Azu6zdLf9y3fxeUhhBglSvnN3jgiNat2xncue4tpUePkj6eMOytug1mfPZ1jRq14ECtw98x66IK7dAcmNIYj2v9kzkOytFR+45iVnaWLoUrwjw8yqhUSv2ccrkc4lko4wg1nmW84HPSxM/Kl6+gnw4BMtGO0RP40ajxQwaPunDhP3BaX8/7IiAwiCv+jAFhEDg8CJ6gyCOGPFOxF0WEiWDan6CcgqD46tVLupTr16Oh5PLy8i6UE7JVrVoDghVdyo9rVnz3/RITO/cr7y/Pc29QlnF/gQFBAf4VHRwcoCgEDcEqOzu7Ro2aRMycDx7u1q3rxCQQk4HX5MQEcKH961yUULCZnsxtf3JxdoEIZv2Gn0+dOpaWnga19J27Nvfo0c/gNNGdO/Y4Ef3zBAAAEABJREFUe/bfzVvWQf0cAmGIeStWrGRi56CbwYNGQgAO1X7wK3D7J08ZveybeSRPGVA9XLlq2b37iRDubNj4CwTjITXrwCpwZnD7L1w8CwF7oR0GBVWBsGnPH9sh85n/ToFjg8AcWiJgVQX/QPg8evTva9ejzbooakplPA6hi7lGGzN6Ehh6zlfT4SZBHNO3z5A+vQcZzBke3iEtPfW3tasVCoWnZ5kRw8dBlc30znv3GgjeYuOmX+HeQzFUs0btSZO0TQ8QgE+cMP3X337YsnU9fA2r9/aSxau4YLlj+27gqD6ZMgaaEgrtrWWL8Pj4WBDo0mVzoYI5dUrEps1rN/7+a3p6Guzt/fCOUAMFUS5d8gP9RQkCm3Vq+3F6nJObpMPICgSxKBcPP71y/PmYpZWJLbBtf1+CWByoSzJiYits29+XIBaHhfDJdiPvo38qaTA27VZmy+ctDHYgL3HYsLzTsPh+cInDhu3jDBF4/ye+Uiqft2hLO3zBxTrY0Kw2fX6HL7iUOGznn4Tff5yv2PK9a+w/XvIolfFT3os9GD9Zg1L7PBjbC0octmwvYLG8K3HYTE8yOZHIMSC3POD4xXJiK2ymJ7kjk52hJIilefYoS2K7eQJs5iHqtnBVpOIM9JbnaaLSv5ojsRE201PVt9ydy4g3LYghiOXYvTKOEZM2/coSG2HjQQcPbXxw53Jm+SoO5ao4yKSG3TQ3XX3RdCZvZjxRwWF/mLzc3ASEharNRd/rh4o1kzfWpG4njOFBhDT5jToaAzl0Cdx5Mi82yFul0eg21E8psJvC56XRFGpCKpKhyFWzqtykhKzE25n2TqK+nwQS22H7QSyPbk+6cykzJ5tlVcQ8Xn/gB7o9GMyloWk+MyRhazQ2iqXav7JB9h2H2ngmQRwUlYotW7bExcVNnTqVICbB8cepyM3N1X+xGDEG2ogK1BMlaCMqUE+UoI2oUKlUUinOJlo8+MSDCvRPlKCNqEA9UYI2ogL1RAnaiAqMnyhBPVGB/okStBEVqCdK0EZUoJ4oQRtRgXqiBG1EBcbjlKCeqED/RAnaiArUEyVoIypQT5SgjahAPVGCNqIC9UQJ2ogK1BMlaCMqUE+UoI2oQD1RgjaiAtszKUE9UYH+iRK0ERWVK1dGPdGANqLizp07UOQRpDhQT1SAcyo6BzVSFNQTFagnSlBPVIjFYrUaR6sqHtQTFeifKEE9UYF6ogT1RAXqiRLUExWoJ0pQT1SgnihBPVGB9TtKUE9UoH+iBPVEBeqJEtQTFagnSlBPVKCeKEE9UYF6ogT1RAXW7yhBPVGB/okSnB/BFM2bN09PTwfPpD+BTLly5fbu3UsQQ+D4vqZo3Lgxy7JQ2In0CA8PJ4gRUE+mGDBgQNmyBeb+8vPz69WrF0GMgHoyRXBwcFhYmH7Ku+++6+3tTRAjoJ6KYfjw4eCTuGUfH5+ePXsSxDiop2IAMUEUxS03aNAgICCAIMbhb3tBzKU0hhEXSjQyf6Z2akOD9VQNo/1HqCm6f0hpXr/v9fMp6lx187f73LmsoNzQUHqB2RSNbWJ6Vd7aXE8/mauHPeEffGwv+HlmbGYG1KqI+vXfeMuf0LVEof2VaYhMTlr08a5Uy4XwCd7pacWkmArBdi16+xHEJCf/SIq5kNFrcnmvcjxyVPzS0/efxLT5sLwPnwzEc9bNjukwqqx/FUfCD3gUj29eEu/kLkExmUW5yvaHNjwivIFHekp9oipXxY4g5lC7qVtWOkt4A4/qd+pcxsFRRhBz8CrvyKsAmE96UmtKXFXsjYB6QkoqPNMTY0bbI8JDeKYn7IxlPrwyGZZ3godXLp1HetKWdVjcCRwe6Ulb1mFxJ3CwvEMsCepJ8GB7JmJJeNXGgnpCLAmP9CQSYXum4OFR/wKWFXZ7ZmxsTPOWYZcvXySlGCzvLIabm/vAAcO8vX3JGwbj8RKJh4fnkMGjyJuHTzGCsN+XOn3m5ISJI9u2b9xvQJe582c+e/YUEq/fuArlDnzqsvUf0OX7lUthYcvW9V26tTpx4mi3Hm1atKrff2DXgwf36bJdvXp5ytSxnTo3HzCoG+RXKPJfZdm+Y1P3D8JPnDzasnWDJcu+bh3+zvoNP+u2UqvV7Ts2Wf3jcv3yLj0j/dsVC/v179yuw3twhvv279LlP3nynxEj+4W3bdSzd7vpn0949CiJS58ZMWX2nGk/rP4WdnLt2hUiTHikJ4jHGXPi8Vu3b0yb/nHduvV//Xnb/8ZNuXPn1vwFEaY3EYslCkXG4ci/NqzbvWvn4ZYtwuctiEhMjIdV9+4nTp4yOjsne8XyX+bMWhQbe3vCxBHcmCoymSwzU7Fnz7Zpn87u2aN/w3feO348UrfPc+fPZGZmtmzxvv6BFiyYde3q5fHjp8G5Va8esnTZXBArl/mLiE/atGm/ZdP+mTPmPXr0cNm387hNpFJpbFwM/H01Z4m/f0UiTPgVj5v1ckT0lSg7O7v+/T708fF9u0GjxQtX9ukzuNitQCLduva2t7d3cXYZPGiko4Pj4cgDkH7o0J9SiRSU5O8fGBgYNHnSjNsxN8EnEW2lk8nOzu7de1Crlu/7+fk3bdoKpPww6QG3wxMnjkD+SpWq6B/l0uULTZq0rB/2jre3z4jh475b8aunpxek//zLyibvtejRva+rq1vNmrVHfzTx9OkTN25e446SlPRg1swFjRo1cXJyIvTwKX4ScHkXUisUbvO0z8Zv3bYBvAvcobqhYTQbBgdX5xbgFpYr55eQEEe0hd2latVqwk64Vb6+ZWHV5SsvK2vVqtbkFt5t1FQul3MuCn4A/xw7XMg5AbVqhULZunLVslOnjqlUqqrB1WGHRFsHvA1H0WWrGlwDPm+8KJoD/CvCL4SYC7ZnGiTPN5lhm+Aq1ebN/fbYscMQu0C4U++tBuBvQkLqFLshqOHlsp0dlICwkJGRDn4CYhf9nCnJz3TLUOpxC3DLGzVscvzEkZ4f9L9yJSo9Pa11q3aFDjF1SgSUj5FHDoCqnBydunbtNXDAcFB/Tk6OXP5SMQ4ODvAJhWn+IfROTKDwrL+Kmb4bijn4g1rV+fNntu/4ffpn43ds/7totlx1gaHlINB2dMx/YS0nO9vdzQMWPDzLgFMpVEFzdXEzeNxmzVpD+Azh/7HjkVBsQYFbKAMUplAQ9+s7JDr6Eihv3fqfnJycoZyFVdnZWS/PJE9Jnh5lSEmBT/E4Y177eFTU+TP/nYKFMmW8wsM7jBk9CWpVSY8eymXaX3lWViaXLSMj4+nTJ/obXow6yy2At0hIvFuxYiVYrhRU5fHjpDq134JCk/sDnUEsZfDQEJKDIk+fOQEeqGhhl5qWumPnZvBGUJ6CRkd/NAH2BiGXRCKBgo8LzDm45aCCsZeg4VM8rjGvfTz66qWIWVP+2Lvj+fOUa9ejd+zcBMLy9SlboUKAs5Pz/j93Q3AD0fe8BTOdnV++5C8SiXbs2JSQcBfq+RAdg6Q4QfTo0Y9l2RXfLwYdQI0P6u0fDusFtS2Dh4a6WKNGTaFES0193qxpq0JrJWLJb2tXR8yeCs4pOfkZNEncjrlRKyQUVnXt0gti/O3bf09LT7sYde77lUveqlu/SuWq5HXA9kyLAOELKGnFd4uWLP0agpsWzcOXLlnNzUI+Y8bcb76dDy1MoLCRIz6Gm6qrOYLPgA0nTh4FpRXU8j6dEgH6I3kl1E9rNm/a9NvIj/qD2iBq/mTyDAjRjB29WZNWn/09EWpw7u4ehVaB65odsXD5dwvHfTwUvoL/GzVyfNv3O8EytBQ8efp489Z1IFwoJcPqvTN82FjymvApHufR+AUrJsbUa+kR0tiDWA1omQSXcPjv/0gJ4reImLFLKxN+wKd4XOu5sX+BsOFTe4FWUtiBXNjwKB5ndB9Wo3u33iWssCPY39cYGt0HYg7Y3xcpsfAsHsf+vgKHZ/E4jl8gcPjln/CFc6GD8ThiSXjmn3CAOoHDM/+kwfJO2GB7AWJJUE+IJeGRniRShjA8GkobeQV4pCexmChScQ5x80iKSxeJCX/g0fNgd1/pvdsKgpjD5eMp9i48qsTwSE89/ueflcFGn3xCEGoe3lV2HutDeAPv5itbOTXGzUvydgdvr7IOBDFCRmrW6f3JD29nDZlV0d6JRwUeH+dTXPtlXHqKGh4Nq3kTTUE7K3+axrQv5hNi58h0GVvOw5tf03HxUU8cz5KUrPHaHpTTBldyE78Wnf610AyqxU2oynUW1aqISzkSGfngwf1+/Qfo5eE09jKP/rLBszKWolvOe0Kgl653kgX2oFZ7VeDprG78bX/y9OXRXFNqcUquKMWrHE5/VQzYnkmFSqWSSqUEKQ7UExWoJ0pQT1Tk5uZyb4oiphH2+HRvDPRPlOBvjgrwT6gnGlBPVGB5RwnaiArUEyVoIypQT5SgjahAPVGCNqIC9UQJ2ogK1BMlaCMqoP0J9UQD2ogK9E+UoI2oQD1RgjaiAvVECdqICtQTJWgjKlBPlKCNqEA9UYI2ogL1RAnaiArUEyVoIypQT5SgjahAPVGCNqKiSpUq2D+TBtQTFbdu3eLmpkZMg3qiAgo71BMNqCcqUE+UoJ6oQD1RgnqiQiwWq/kz2guPQT1Rgf6JEtQTFagnSlBPVKCeKEE9UYF6ogT1RAXqiRLUExVYv6ME9UQF+idKUE9UoJ4oQT1RgXqiBPVEBeqJEtQTFagnSlBPVGD9jhL+zo/ABzp06JCbR0ZGBtFOUsCoVCo3N7dDhw4RxBA4vq8pAgICHj169Pz5c05VICaWZZs3b04QI6CeTDF48GBvb2/9FF9f3969exPECKgnU9SvX79GjRr6KWFhYZUqVSKIEVBPxTBs2DDwSdyyl5dXr169CGIc1FMx1KxZMzQ0lFuuXr06fCWIcVBPxTNw4EAfH58yZcr07duXICYppr3g0KYHcVeyVDkaE40vrzN35WvNe6nhZiB8pbV0Uyq+8ubFHr3YHNobwxSzCxrrFZuHm36yWCRiIpKQsgHyTh9VIKYOZ1xPkVuSbp3PCAxxDq7nJJIYfjuWOxONgf3mTV6pXWV4asz8OTA12n8vNslf1l2h/toCG+t9K2Av/XvEHVjPUty0nXpnWHjnBSh4u4uqR6QhrP5t4Kbq1D9YwaMbmyC0QLreQQtMzmlEu0W1orfb/MlCjW5r8ARMkEvu3kqNuZDq7CLtNTnAWC6jetq8OD71uarP5MoEQfTY9V1cbg47ZJbhSq7h+On+3YxnD1FMiAG6jKmozNEc25VkcK1hPf33Z4q9C49mYUd4hbuvPO5KtsFVhvWUna6WSHkzPzzCM5xcJUql4bnnDfcvUOYQDYt6QgyTm8vkZhsOu7G/CmJJUE+IJTEcP4lEVG1cSOkEtMEY0Ydh/8SyBLvZIcYAbRhrtsTyDrEkxvSEpR1ilLzyzvAqY/0LsLRDTGNO/MSIUFGIUUzET4b9U17PACzyECMYrxefNFIAABAASURBVKwZq9/ha1SIcUSMee0FCGIaDTGvvEMQ42iMhtdG6neM2e3jnbu2XLtuDUH4xMyIKZMmf0TeIIb1lBfAE7Po1XNA7Vp1yZsiLu5O774dyBth564tc+fPJK/KK5/qax4XaNKkZevW7UznmTX70/1/7ibmYsTdWCx+6ttnMHmD3Lx1jbwpbt58rWO98qm+5nGBli3Ci80DR6lfvyExF/PKO/PRlXfwq+rWo01Cwt0hQ3s2bxk2dHjvvw78Aelnz52Gr9HRl3SbXL9xFVJOnzkJy1evXp4ydWynzs0HDOr2/cqlCoWCy5Oekf7tioX9+ndu1+G9CRNH7tu/CxJ/+XXV/AWzHj1Kgs23btsADgAWYA8fTxgOC336dty9ZxucwKAhPVq2bjBm3JAbejcGTmb02MFt2zeGz23bN+rqsV26tYKt4BJgkw6dmsKv9tmzp5A+fuKIAwf3Hjy4D/Z86/YNExagOdXY2BjtJZ8+0aPn+8NG9CF53uubb+fDqYa3bTRyVH84B25vRY9rzEQsyy5dNrf7B+Fw4Wt++g52DpskJz8jBcs7sDOcFVx4vwFdwO1xVwc5HyY9WLhoTsfOzQg95raPv07nAqlUmgGWXb7gk0kzIg+dbdqk1YKFs8Ggb9Wt7+zkfOx4pC7niRNHIKV+2Dv37idOnjI6Oyd7xfJf5sxaFBt7e8LEEdx4SwsWzLp29fL48dN+/Xlb9eohYDgw65DBo3r3Gujj43vk8LkPevTjZqZb8d2iQQNHwBFrhtT5cc3yZd/Mmzol4sCfp+QyOZwMd8RDh/+CuxtcpdrG9XuGDR0Delrx/WLdaW/evFYkEu3aefi3X7ZfiY769bcfIH3ZktVw3DZt2sOxYEMTF05/qmvXr4HwYNLEz2H5u+8Xnz3778f/mzpv7rft2nUBbXE/sELHNWEikOkfe3eMG/vJqlXr7e0dfvr5e+19FRW4s6DIadM/rlu3Ppzb/8ZNuXPn1vwFEZD+137tsT6ZPOOP3UcJPcbDIZGpjV4VlUoFt7ZGjVrQShHepgP4gJiYm2KxuHnzNseOH9ZlA221bPk+pB869KdUIgUz+fsHBgYGTZ4043bMzRMnj0KeS5cvQBAAmvP29hkxfNx3K3719PQyeFDYFUgWjtisSSv47Xbq1KNG9RCJRAKbw9E5P7R//67ateuO//hTd3cPyDxk0Khdu7akpCRzeyhfvkL/fh+CxD09y9QPa3jr1nViDjSnyjXbQB7QVvVq2leNZ8yYu3Dh93AydUPDOnfqUTW4+n9nTxXduQkTgRtr8l6LZk1bubq49us7xMHRsejm0Vei7Ozs4OpA2W83aLR44co+rxefGGt/MuKfRPDvtRoNqlXLfy/b2dkFPsFjwWezZq3BUXHeG/z8vXsJLVu8T7Se/BLkd3V14zbx9S1brpzf5SsXYblWrdAtW9evXLXs1KljIFMwN6w1eMQKFQK5BUcnJ/gMqpj/co69nT1sqFQqoVyIvnoJhKLbBH6vkMgdCAgOrq5bBaetUGQQc6A/1eAqLw8Ev/QdOzYNHNwdih74g6L5+Qt962PMRGq1+u7d2Jo1a+tyNnmvZdHNQ2qFZmdnT/tsPDgzcHWwH5AveVXypGFOfxVWrX07lbwGBvUbWqceOIZjxw6DAz9+4oiXl3dISB2SpzawI1hTP3NKXgQAZdaePdsijxyAW+Xk6NS1a6+BA4YbnMq3kIcv9BUAScFthuKAKxFeHujF/WNe7ydEf6oyuZxbADV/Ov1jUPvwYWNDQ8PANY77eKjBnRszUYYiA26Vg8NLn6TTnD5gcChPwfKrf1wOsVe9txoMHjSSM/4rYKL6/0bbx+GGQZEHXhpiFwieWrfKr8p6eJaBHzeEGvqZXV20dnFxdgEvDW4cAnmQ4Lr1Pzk5Off8oD8xH3D4Dg4ObVq3h1JJP71cWT9iCV7hVMFV37hxddHC7+EGcymgG68y3kVzGjORg70DyQswdIkpKc8MHguKOfiDPZw/f2b7jt+nfzZ+x/a/iaUx0r/Aat0LWjRrA+4d6iBQ/E+fNodLrBRU5eDf++rUfkvnVMCH+/n5p6alHj78V7u2nUEKYE34g0jIdCXLNJUqBUMtTOfq4TY8fHgfwh3y2rzaqaamPodPnYDgquGvYqCBV2+NmQj8H5z/3bt3dDlPnvqn6OZRUedzlDmgpzJlvMLDO/j6loP6Y9Kjhwa1+zoYq98xhLHKQxco6eH6oRYdFFQZ4kousUePfuD5oaoFZXxiYvwPq7/9cFiv2LgYiVjy29rVEbOnwi8eKsBQeb4dc6NWiHb0HDAl1HhPnDgK+emPPnzo2JMnj0LzHRzuypWo2XOmTZw8CspB01tBnH79evSFi2dTDEU2HK92qoEBQSCIzVvWpaWnQQPH8hULIVSH21z0uMZMBNkaNWwCUoPmGCj4IDxKT08renoQOEbMmgLVwOfPU65dj96xcxMIy9enrFwuh6jj3LnTF6POEUtgWE/sKzSQU9OsaWv44bZo/rKpDUqKn9ZshsB55Ef9ITKNunQearBQ5Ds6Os6OWPj06WOIKqB9ZdOWtaNGju/YoRts8s7bjeFuzZg5+XDkAfpDg9tYvWrD5csXu3ZvDdVviLi/nLNE/iKaMUbH9t3gB/bJlDF3Ym8by/Nqpwq1rc+mf3nt+pXOXVpM/3wChAFQLQUNQXNUoeMaMxFkg6p0rVp1oWlqwMCu8fFxPbprBxWSFBy/BIrd9u26QpMKXDg0NEC8tXTJai6269f3Q5DsjC8mEUtgeDyM3+bc1bBM9/EBBOE94LEeP06CdgTu66bNazds+PmPPUeJ1TiyJen+LcVHCw2UyziemOABAY0Y1W/7jk0QjUUeOQi1S3ByxKqY+74U9lgxSMdOzYytmjo1ovG7zYgtGDxoRGpqysGDe+GpgJeXT9cuvaCOSayKue9L4cucBtm48Q9jqyCyIbYDHteQNwjDWL9/QWkA2hsJwnUfN+v5nQa7jyOvhNHnd4RBRSGGMXv8Ag37OuPuIiUcje6jCMbGV2EYjMkRY5j7PBjfv0NeDWPtBeickFcB63eIJcH2J8SSGPZPUplIJEEPhRhGLCHG5GFMTxqWGB5gGkGyFUqp3Jz3ESrWccxOQ/+EGCb1idqrvJ3BVYb1FNaijFRK/l5vRtdHpJRw8/xTZZa64/DyBteamq9szYw7cgfSZTROl4vkc2z7g4TrmSPmVRSLDU/vU8x8ir/Nic1IZWFbda7RFqmCM7VpNHpdGRjGaEMqHBda4U0cPH/+OI3h/ehSDM47WGSyuQJnZWCHRV7AKJQBnmeyJjIYmpOQOoOpiRUZETz7MniC+Rdu1MKmpkksfES9nRg9GbFUo1ETmT0zdLYp/8IU29KkzFJeOJaqpHu3UUPfE++1Z7A0kauIMc04L4Pcv38/I0NRtWqwwbUG753+9I3crTe8Le00okUUZXJL/SPmLWpMbEdjHYmMCQp19K1QTDev4tufZPayd8K9SOlm06bDz3MSm3R/lyAmwfZMKnJzcw2+6YsUAm1EBeqJErQRFSqVihtqBzENvi9FBeqJEtQTFVjeUYI2ogL1RAnaiArUEyVoIypQT5SgjahAPVGCNqIC9UQJ2ogK1BMlaCMqsP2JEtQTFeifKEEbUYF6ogRtRAXqiRK0ERUYP1GCeqIC/RMlaCMqUE+UoI2oQD1RgjaiAvVECdqICozHKUE9UYH+iRK0ERV+fn7on2hAPVGRkJCgVqsJUhyoJyqgsOMmf0ZMg3qiAvVECeqJCtQTJagnKsRiMcZPNKCeqED/RAnqiQrUEyWoJypQT5SgnqhAPVGCeqIC9UQJ6okKrN9RgnqiAv0TJagnKlBPlKCeqEA9UYJ6ogL1RAnqiQrUEyWoJypQT5QwOBOnCVq1asV181UoFLDg4OBA8ibD3bt3L0EMgf7JFF5eXjdv3hSJ8ketTU9PZ1m2efPmBDECju9risGDBzs5OemnuLm59e/fnyBGQD2ZIjw8PDi4wBxA1apVq1u3LkGMgHoqBnBRzs7O3LKrqys6J9OgnoqhcePGNWrU4JYDAgIaNWpEEOOgnopn4MCBHh4ejo6Offr0IYhJSlR7wb/7n9y/lZWWolYpNayaZdU0cyhqGMKYNgHDEO3eNKxYLDGdTcMWnLrQCNomCIZIpIyDi9g30K75Bz6kpFAS9HTjQurpP5IVaWpGxIilIqmdVGoHC2JGU7z35QRAozs2b/JVk1k0mrxMFBOPgtY1uUp1jkKlVqpZNZE7MrXfdX27bRkicIStp8y0rN8XJWUr1HJnmW81DydXeyJAVCrVvUtPMp/niMWkRW+v4LdciWARsJ72/Xw/LjrLycMusF5ZUiK4F/04NUnh4SPpMyWQCBOh6mntl3czM9hqTQNIiePWiQRGww7/uhIRIIKs3+354UFJFRMQ3NhfJJf+HHGXCBDh+aff5sRlZWqqNSmZYtIRd/6hSqEcMTeICAqB+afdq+9npqtLvJiAivXKMlIRFOtEUAhJT8mPshKvZ1VvXpGUDqo0rJD2LPfE3kdEOAhJT1uXPHD0tCOlCZ9gt6jIdCIcBKOnS8eTodW7YklpGqDEK9BdJCb7frxHBIJg9HTu4HMHdznhK9v/WLBwuVWe7rn7OcddyyYCQTB6yspg/Wp7k9JH2WDtQ5hr/z0nQkAYejq67bFIzMjkpbR3ssROfOV4GhECwrhDd69lMBKah7avyNkLe/89u/Pho5iyPpVDa7V6r2FveEoM6es2T4cmurfqvL95x+ycnMyACrXah48NqBACq+Drhm1fxMSeg00a1u9GrImdsyz1qTCKPGH4p2wFa+ciI9bhwqUDm3fO8StXdfrEnW1bf3Ts1Kbd+5dyq0QiSXzilfNRf3486tevv/hHIpVt2jGbW7Vl11dPnyWOHLxiUJ/5SY9jb9w6SayGk6dcrRZGs7Mw9MSqiZ2DtYLx/87vDgqo263jFGcnjypBYeEtR5w8szU9I5lbC36oV9fPPT3Ki8WSt2qHP3kaDympaU8uRR9q3ngA+CoXZ88O4WOlEis2ZNi72QllcBfBxOMSB6sUzSzLxiVcDq7yti4FJKXRsHF3o7iv3l6BcrkDt2xnp+1InpmVlpxyHxZ8vF+2rFYoX51YDUcXe6IRxpMxgUS4jIhREWuQm6tUq1V/HVoFf/rp6Yp8/8QwBn5yisxU+JTLHHQpMpkVu16Bkhii4UI6niMMPYlEbE4OBKQuxNLIZHYgi3qh7WrXbKGfDgWcia0cHbRd3pSqlzFydo6CWI30lEyNALSkRRh6ktmJlAprDR9QrmxwVnZ65aB63NfcXNWzlPturqb6dLu7lYPPuwmXuWIONrl95z9HR3diHbJSlGIxEQTCiJ9cPCTKLGvpqV3rj6Kv/3Pm/B5tLBUftX7LZz/8MgbKQRObuLl6B/rXORC5+vGTeJUqZ8PWGcSahZEiOUtuLwxBCUNPletG/S+LAAAC40lEQVQ45eZYq4ZTMSB0wkdrIQCPmP/+D7+Oy8rOGNJvoVRaTHWyT/eZ/n41l60c+NmXzR3sXRq81YlYLV7OyVB6+VmrucSyCKY/3feTY3yqe3qWs3wIxX+iD8YNjPB3cRWApATTXuDmLX0Wm0pKH3EXkmQOjCDERAQ0Xs8HE8r9MDXeRIYz53b/ceBbg6sgxDFWfvXu9kVI9abEQkD49dP6SQZXQUAmFksN1vm7d5xat3YbYoTM5KyG7T2IQBBS//GNCxIyUtXBjf0Nrs3OVmRmGXZgisw0RwfDBaWTowc0GRDLkZzywGB6dnaGnZ2TwVWODm66JtNCxJ1/qM5SDvtKML3IBfY+AkRRvsGeHhVKSxQFkdOI+f4ymTAKOyK49xHaD/F5ePMZKR1cP3K3xjtOAhITEZyeAmo6hzZzuXoojpR0rkXG+QTIW/TyJYJCkO8HP0nM2rzsfkirEvuiy42jd+u3ca/XUjBhuA5B9nj0qmDfoI37fwfinH3sA2oL7BdsmsexyU9iU/2r2gtRTETo46v8MC2GVTOeAS7eQYK0vj6pjzMeXn/G5rKt+nsHhwq1wiH48Z/+3pB0+2IGPD6zc5Z6Brq7ejkQQaFIy3x6J1XxPIfN1ZSvbNd1tB8RMiVkfLoTux7fisrISmfharSthiImrwPaywzaweMIo/+ITTuO2Iuv+sscmheDguWNXqfJyyPSjhfG7UqTv46weTl0m8NxWa6jkjaB4fbDcFnzujHl5RSJYFfax5HgjWCXUjkTWNOhTf+S8GphSZsfIe566p2oLEWqWpXNKpUvL02UN7Zcnh7yxpljtSksm3/LRSKGZTV6qtKIGBGr4WSUrx6xhFHn5q0SabNp5aLdWKs2sZjh+ndLZSKVkhWJuSHo8pTIELFYuwdGJFKrWImUyc3VSOUiqYw4OoshTqpa342UIHC+DcSS4HwbiCVBPSGWBPWEWBLUE2JJUE+IJUE9IZbk/wAAAP//mTVPcwAAAAZJREFUAwBE1QOE6K5HJQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x1455ae0e0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc30bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
